{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "980erA1glzfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdjXEmHMh-AO",
        "outputId": "4527a22f-39d6-465f-9baf-e4061053cb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef83IdZufDTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb8489f-4b1f-49b7-ceb0-9523d7f538e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-A7-A13E-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-AY-A8YK-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-HE-7128-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-G9-6356-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-NH-A8F7-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-HE-7129-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-HE-7130-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-38-6178-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-G2-A2EK-01A-02-TSB.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-FG-A87N-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-B0-5710-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-G9-6348-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-MH-A561-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-B0-5711-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-CH-5767-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-A7-A13F-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-BC-A217-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-50-5931-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-DK-A2I6-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-UZ-A9PN-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-21-5784-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-E2-A1B5-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-E2-A14V-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-AR-A1AS-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-F9-A8NY-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-G9-6336-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-49-4488-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-21-5786-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-B0-5698-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-G9-6362-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-G9-6363-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-KB-A93J-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-AR-A1AK-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-XS-A8TJ-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-18-5592-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-RD-A8N9-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/TCGA-UZ-A9PJ-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-2Z-A9J9-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-44-2665-01B-06-BS6.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-GL-6846-01A-01-BS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-CU-A0YN-01A-02-BSB.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-AC-A2FO-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-69-7764-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-FG-A4MU-01B-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-AO-A0J2-01A-01-BSA.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-A6-6782-01A-01-BS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-EJ-A46H-01A-03-TSC.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-HC-7209-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-ZF-A9R5-01A-01-TS1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-HT-8564-01Z-00-DX1.xml\n",
            "/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/TCGA-IZ-8196-01A-01-BS1.xml\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import skimage.draw\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from scipy.io import savemat\n",
        "\n",
        "\n",
        "def binary_mask_from_xml_file(xml_file_path, image_shape=(1000, 1000)):\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    def vertex_element_to_tuple(vertex_element):\n",
        "        col = float(vertex_element.get('X'))\n",
        "        row = float(vertex_element.get('Y'))\n",
        "        return round(row), round(col)\n",
        "\n",
        "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "    iii = 1\n",
        "    for region in root.iter('Region'):\n",
        "        vertices = map(vertex_element_to_tuple, region.iter('Vertex'))\n",
        "        rows, cols = np.array(list(zip(*vertices)))\n",
        "\n",
        "        rows[rows >= 1000] = 999\n",
        "        cols[cols >= 1000] = 999\n",
        "        rr, cc = skimage.draw.polygon(rows, cols, mask.shape)\n",
        "\n",
        "        '''\n",
        "        # To add the nuclear boundary as a separate class\n",
        "        mask[rr, cc] = 2\n",
        "        mask[rows, cols] = 1\n",
        "        '''\n",
        "        mask[rr, cc] = 1\n",
        "\n",
        "        iii += 1\n",
        "\n",
        "    return mask\n",
        "\n",
        "# TRAIN SET\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Annotations/*'  # Annotations Folder\n",
        "folder2 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_train/Tissue Images'  # Images Folder\n",
        "\n",
        "# Train Output Folders\n",
        "labelled_images = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/images'\n",
        "os.makedirs(labelled_images, exist_ok=True)\n",
        "labels = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/masks'\n",
        "os.makedirs(labels, exist_ok=True)\n",
        "\n",
        "labels_list = []\n",
        "IMGS = glob.glob(folder1)\n",
        "indexing = 0\n",
        "for mask_path in IMGS:\n",
        "    print(mask_path)\n",
        "    img_path = os.path.join(folder2, mask_path.split('/')[-1].split('.xml')[0] + '.tif')\n",
        "\n",
        "    image = Image.open(img_path)\n",
        "    image = np.array(image)\n",
        "    save_path1 = os.path.join(labelled_images, 'image_' + str(indexing) + '.jpg')\n",
        "    cv2.imwrite(save_path1, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    image_label = binary_mask_from_xml_file(mask_path)\n",
        "    save_path2 = os.path.join(labels, 'mask_' + str(indexing) + '.jpg')\n",
        "    cv2.imwrite(save_path2, image_label)\n",
        "    labels_list.append(image_label)\n",
        "\n",
        "    indexing += 1\n",
        "\n",
        "\n",
        "masks = np.array(labels_list)\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data', 'train_masks.mat')\n",
        "mdic = {\"data\": masks, \"label\": \"labels\"}\n",
        "savemat(path, mdic)\n",
        "\n",
        "\n",
        "# TEST SET\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Annotations/*'  # Annotations Folder\n",
        "folder2 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/MoNuSeg_test/Tissue Images'  # Images Folder\n",
        "\n",
        "# Test Output Folders\n",
        "labelled_images = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/images'\n",
        "os.makedirs(labelled_images, exist_ok=True)\n",
        "labels = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/masks'\n",
        "os.makedirs(labels, exist_ok=True)\n",
        "\n",
        "labels_list = []\n",
        "IMGS = glob.glob(folder1)\n",
        "indexing = 0\n",
        "for mask_path in IMGS:\n",
        "    print(mask_path)\n",
        "    img_path = os.path.join(folder2, mask_path.split('/')[-1].split('.xml')[0] + '.tif')\n",
        "\n",
        "    image = Image.open(img_path)\n",
        "    image = np.array(image)\n",
        "    save_path1 = os.path.join(labelled_images, 'image_' + str(indexing) + '.jpg')\n",
        "    cv2.imwrite(save_path1, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    image_label = binary_mask_from_xml_file(mask_path)\n",
        "    save_path2 = os.path.join(labels, 'mask_' + str(indexing) + '.jpg')\n",
        "    cv2.imwrite(save_path2, image_label)\n",
        "    labels_list.append(image_label)\n",
        "\n",
        "    indexing += 1\n",
        "\n",
        "masks = np.array(labels_list)\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data', 'test_masks.mat')\n",
        "mdic = {\"data\": masks, \"label\": \"labels\"}\n",
        "savemat(path, mdic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlRYn1TAkAvZ",
        "outputId": "f8559c16-0215-4473-9ff9-a1a22f7914ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to generate the unlabeled image patches for self-supervision from the official train set of MoNuSeg\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "\n",
        "folder = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/train_images/*'\n",
        "\n",
        "# directories for images\n",
        "OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/unlabelled_img_patches'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "PATCHES = []\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split('.jpg')[0])):\n",
        "    image_number = int(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    ximg = transforms.ToTensor()(img)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = ximg.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(img.shape[2], -1, size, size)\n",
        "    patches = torch.permute(patches, (1, 2, 3, 0))\n",
        "    patches = patches.numpy()\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        save_path = os.path.join(OUT_FOLDER, 'image_' + str(indexing) + '.jpg')\n",
        "        cv2.imwrite(save_path, np.uint8(255*patches[i, :, :, :]))\n",
        "        indexing += 1\n",
        "    print('{} patches are created'.format(indexing))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx6g1cGTkYHu",
        "outputId": "105719ed-1584-4073-d1d6-4a4219e77aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45 patches are created\n",
            "90 patches are created\n",
            "135 patches are created\n",
            "180 patches are created\n",
            "225 patches are created\n",
            "270 patches are created\n",
            "294 patches are created\n",
            "339 patches are created\n",
            "384 patches are created\n",
            "429 patches are created\n",
            "474 patches are created\n",
            "489 patches are created\n",
            "534 patches are created\n",
            "579 patches are created\n",
            "624 patches are created\n",
            "669 patches are created\n",
            "714 patches are created\n",
            "759 patches are created\n",
            "804 patches are created\n",
            "849 patches are created\n",
            "894 patches are created\n",
            "939 patches are created\n",
            "984 patches are created\n",
            "1029 patches are created\n",
            "1074 patches are created\n",
            "1119 patches are created\n",
            "1164 patches are created\n",
            "1209 patches are created\n",
            "1254 patches are created\n",
            "1299 patches are created\n",
            "1344 patches are created\n",
            "1389 patches are created\n",
            "1434 patches are created\n",
            "1479 patches are created\n",
            "1524 patches are created\n",
            "1548 patches are created\n",
            "1593 patches are created\n",
            "1638 patches are created\n",
            "1683 patches are created\n",
            "1728 patches are created\n",
            "1743 patches are created\n",
            "1788 patches are created\n",
            "1833 patches are created\n",
            "1878 patches are created\n",
            "1923 patches are created\n",
            "1968 patches are created\n",
            "2013 patches are created\n",
            "2058 patches are created\n",
            "2103 patches are created\n",
            "2127 patches are created\n",
            "2172 patches are created\n",
            "2217 patches are created\n",
            "2262 patches are created\n",
            "2307 patches are created\n",
            "2352 patches are created\n",
            "2397 patches are created\n",
            "2442 patches are created\n",
            "2487 patches are created\n",
            "2532 patches are created\n",
            "2577 patches are created\n",
            "2622 patches are created\n",
            "2667 patches are created\n",
            "2712 patches are created\n",
            "2757 patches are created\n",
            "2802 patches are created\n",
            "2847 patches are created\n",
            "2892 patches are created\n",
            "2937 patches are created\n",
            "2982 patches are created\n",
            "3027 patches are created\n",
            "3072 patches are created\n",
            "3117 patches are created\n",
            "3162 patches are created\n",
            "3207 patches are created\n",
            "3252 patches are created\n",
            "3297 patches are created\n",
            "3342 patches are created\n",
            "3387 patches are created\n",
            "3432 patches are created\n",
            "3477 patches are created\n",
            "3522 patches are created\n",
            "3567 patches are created\n",
            "3612 patches are created\n",
            "3636 patches are created\n",
            "3681 patches are created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to split the test data of the official MoNuSeg into train-test split for our downstream\n",
        "# histopathological image segmentation\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "def split_data(images, labels, ratio):\n",
        "    # Ensure both arrays have same length before permutation\n",
        "    min_len = min(images.shape[0], labels.shape[0])\n",
        "    idxs = np.random.RandomState(2023).permutation(min_len)\n",
        "\n",
        "    split = int(min_len * ratio)\n",
        "    split_1 = idxs[:split]\n",
        "    split_2 = idxs[split:]\n",
        "\n",
        "    # Apply indices to both images and labels\n",
        "    return images[split_1], images[split_2], labels[split_1], labels[split_2]\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/test_images/*'\n",
        "\n",
        "# directories for images and labels\n",
        "TRAIN_OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/full_size_images'\n",
        "os.makedirs(TRAIN_OUT_FOLDER, exist_ok=True)\n",
        "TEST_OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_masks'\n",
        "os.makedirs(TEST_OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          IMAGES and LABELS                                    ')\n",
        "print('===========================================================================')\n",
        "\n",
        "IMAGES = []\n",
        "IMGS = glob.glob(folder1)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (256, 256))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    IMAGES.append(img)\n",
        "IMAGES = np.array(IMAGES)\n",
        "print(f'number of images: {IMAGES.shape[0]}')\n",
        "\n",
        "label_file = loadmat('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/test_masks.mat')\n",
        "all_labels = label_file['data']\n",
        "LABELS = []\n",
        "for i in range(all_labels.shape[0]):\n",
        "    mask = all_labels[i]\n",
        "    print(np.unique(mask))\n",
        "    LABELS.append(mask)\n",
        "LABELS = np.array(LABELS)\n",
        "print(f'number of images: {LABELS.shape[0]}')\n",
        "\n",
        "train_imgs, test_imgs, train_lbls, test_lbls = split_data(IMAGES, LABELS, 0.8)\n",
        "\n",
        "\n",
        "for i in range(train_imgs.shape[0]):\n",
        "    TRAIN_OUT_IMAGE_PATH = os.path.join(TRAIN_OUT_FOLDER, 'image_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(TRAIN_OUT_IMAGE_PATH, cv2.cvtColor(train_imgs[i], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print(f'Number of training images: {train_imgs.shape[0]}')\n",
        "\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train', 'full_size_labels.mat')\n",
        "mdic = {\"data\": train_lbls, \"label\": \"train_labels\"}\n",
        "savemat(path, mdic)\n",
        "print(f'Number of training labels: {train_lbls.shape[0]}')\n",
        "\n",
        "for i in range(test_imgs.shape[0]):\n",
        "    TEST_OUT_IMAGE_PATH = os.path.join(TEST_OUT_FOLDER, 'image_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(TEST_OUT_IMAGE_PATH, cv2.cvtColor(test_imgs[i], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print(f'Number of testing images: {test_imgs.shape[0]}')\n",
        "\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test', 'full_size_labels.mat')\n",
        "mdic = {\"data\": test_lbls, \"label\": \"test_labels\"}\n",
        "savemat(path, mdic)\n",
        "print(f'Number of testing labels: {test_lbls.shape[0]}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1RqrIjK3kpn",
        "outputId": "1435b252-53b7-45e1-a2c6-d8da4a9b749a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "                          IMAGES and LABELS                                    \n",
            "===========================================================================\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "number of images: 80\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "[0 1]\n",
            "number of images: 14\n",
            "Number of training images: 11\n",
            "Number of training labels: 11\n",
            "Number of testing images: 3\n",
            "Number of testing labels: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to obtain the test image-label patches form the full-scale image-labels\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_images/*'\n",
        "\n",
        "# directories for images and labels\n",
        "OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/images'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          IMAGE PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder1)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    ximg = transforms.ToTensor()(img)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = ximg.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(img.shape[2], -1, size, size)\n",
        "    patches = torch.permute(patches, (1, 2, 3, 0))\n",
        "    patches = patches.numpy()\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        save_path = os.path.join(OUT_FOLDER, 'image_' + str(indexing) + '.jpg')\n",
        "        cv2.imwrite(save_path, cv2.cvtColor(np.uint8(255 * patches[i, :, :, :]), cv2.COLOR_RGB2BGR))\n",
        "        indexing += 1\n",
        "print('Number of image patches in total: {}'.format(indexing))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          LABEL PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "label_file = loadmat('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_labels.mat')\n",
        "all_labels = label_file['data']\n",
        "LABEL_PATCHES = []\n",
        "indexing = 0\n",
        "for i in range(all_labels.shape[0]):\n",
        "    xlabel = transforms.ToTensor()(all_labels[i, :, :])\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = xlabel.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(-1, size, size)\n",
        "    patches = np.uint8(255 * patches.numpy())\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        LABEL_PATCHES.append(patches[i, :, :])\n",
        "        indexing += 1\n",
        "    print('{} label patches are created'.format(indexing))\n",
        "LABEL_PATCHES = np.array(LABEL_PATCHES)\n",
        "print('Number of label patches in total: {}'.format(LABEL_PATCHES.shape[0]))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                            Now Saving the patches                         ')\n",
        "print('===========================================================================')\n",
        "\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test', 'label_patches.mat')\n",
        "mdic = {\"data\": LABEL_PATCHES, \"label\": \"test_labels\"}\n",
        "savemat(path, mdic)\n",
        "print(f'Number of training labels: {LABEL_PATCHES.shape[0]}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ZAMdl73rsM",
        "outputId": "f970f5ee-504d-4751-9ea6-c93a67560751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "                          IMAGE PATCHES                                    \n",
            "===========================================================================\n",
            "0\n",
            "1\n",
            "2\n",
            "Number of image patches in total: 135\n",
            "===========================================================================\n",
            "                          LABEL PATCHES                                    \n",
            "===========================================================================\n",
            "144 label patches are created\n",
            "288 label patches are created\n",
            "432 label patches are created\n",
            "Number of label patches in total: 432\n",
            "===========================================================================\n",
            "                            Now Saving the patches                         \n",
            "===========================================================================\n",
            "Number of training labels: 432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to obtain the train image patches from the full scale train images\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train//full_size_images/*'\n",
        "\n",
        "# directories for images and labels\n",
        "OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/images'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          IMAGE PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder1)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    ximg = transforms.ToTensor()(img)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = ximg.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(img.shape[2], -1, size, size)\n",
        "    patches = torch.permute(patches, (1, 2, 3, 0))\n",
        "    patches = patches.numpy()\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        save_path = os.path.join(OUT_FOLDER, 'image_' + str(indexing) + '.jpg')\n",
        "        cv2.imwrite(save_path, cv2.cvtColor(np.uint8(255 * patches[i, :, :, :]), cv2.COLOR_RGB2BGR))\n",
        "        indexing += 1\n",
        "print('Number of image patches in total: {}'.format(indexing))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          LABEL PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "label_file = loadmat('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/full_size_labels.mat')\n",
        "all_labels = label_file['data']\n",
        "LABEL_PATCHES = []\n",
        "indexing = 0\n",
        "for i in range(all_labels.shape[0]):\n",
        "    xlabel = transforms.ToTensor()(all_labels[i, :, :])\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = xlabel.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(-1, size, size)\n",
        "    patches = np.uint8(255 * patches.numpy())\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        LABEL_PATCHES.append(patches[i, :, :])\n",
        "        indexing += 1\n",
        "    print('{} label patches are created'.format(indexing))\n",
        "LABEL_PATCHES = np.array(LABEL_PATCHES)\n",
        "print('Number of label patches in total: {}'.format(LABEL_PATCHES.shape[0]))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                            Now Saving the patches                         ')\n",
        "print('===========================================================================')\n",
        "\n",
        "\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train', 'label_patches.mat')\n",
        "mdic = {\"data\": LABEL_PATCHES, \"label\": \"train_labels\"}\n",
        "savemat(path, mdic)\n",
        "print(f'Number of training labels: {LABEL_PATCHES.shape[0]}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pN_KmgJ3wdb",
        "outputId": "eec799b4-d0c2-4824-9427-02ebaa72bfc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "                          IMAGE PATCHES                                    \n",
            "===========================================================================\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Number of image patches in total: 11\n",
            "===========================================================================\n",
            "                          LABEL PATCHES                                    \n",
            "===========================================================================\n",
            "144 label patches are created\n",
            "288 label patches are created\n",
            "432 label patches are created\n",
            "576 label patches are created\n",
            "720 label patches are created\n",
            "864 label patches are created\n",
            "1008 label patches are created\n",
            "1152 label patches are created\n",
            "1296 label patches are created\n",
            "1440 label patches are created\n",
            "1584 label patches are created\n",
            "Number of label patches in total: 1584\n",
            "===========================================================================\n",
            "                            Now Saving the patches                         \n",
            "===========================================================================\n",
            "Number of training labels: 1584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.io import savemat\n",
        "\n",
        "folder = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GLaS_data/*' # path for the original GlaS dataset\n",
        "\n",
        "train_images = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/train_images'\n",
        "os.makedirs(train_images, exist_ok=True)\n",
        "train_labels = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/train_masks'\n",
        "os.makedirs(train_labels, exist_ok=True)\n",
        "\n",
        "IMGS = glob.glob(folder)\n",
        "for img_path in IMGS:\n",
        "    image_type = img_path.split('/')[-1].split('_')[0][:5]\n",
        "    if image_type == 'train':\n",
        "        is_annotation = img_path.split('/')[-1].split('_')[-1].split('.bmp')[0]\n",
        "        if is_annotation != 'anno':\n",
        "            img_num = int(img_path.split('/')[-1].split('_')[1].split('.bmp')[0])\n",
        "            # print(image_type+str(img_num))\n",
        "            print(img_num - 1)\n",
        "\n",
        "            img = Image.open(img_path)\n",
        "            img = np.array(img)\n",
        "            save_path = os.path.join(train_images, 'image_' + str(img_num-1) + '.jpg')\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        else:\n",
        "            img_num = int(img_path.split('/')[-1].split('_')[1])\n",
        "            # print(image_type+str(img_num))\n",
        "            print(img_num - 1)\n",
        "\n",
        "            mask = Image.open(img_path)\n",
        "            mask = np.array(mask)\n",
        "            mask[mask != 0] = 1\n",
        "            save_path = os.path.join(train_labels, 'label_' + str(img_num - 1) + '.jpg')\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(255 * mask, cv2.COLOR_RGB2BGR))\n",
        "print('='*30)\n",
        "\n",
        "test_images = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/test_images'\n",
        "os.makedirs(test_images, exist_ok=True)\n",
        "test_labels = './content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/test_masks'\n",
        "os.makedirs(test_labels, exist_ok=True)\n",
        "\n",
        "IMGS = glob.glob(folder)\n",
        "indexing = 0\n",
        "for img_path in IMGS:\n",
        "    image_type = img_path.split('/')[-1].split('_')[0][:5]\n",
        "    if image_type == 'testA':\n",
        "        is_annotation = img_path.split('/')[-1].split('_')[-1].split('.bmp')[0]\n",
        "        if is_annotation != 'anno':\n",
        "            img_num = int(img_path.split('/')[-1].split('_')[1].split('.bmp')[0])\n",
        "            # print(image_type+str(img_num))\n",
        "            print(img_num - 1)\n",
        "\n",
        "            img = Image.open(img_path)\n",
        "            img = np.array(img)\n",
        "            save_path = os.path.join(test_images, 'image_' + str(img_num - 1) + '.jpg')\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "            indexing += 1\n",
        "        else:\n",
        "            img_num = int(img_path.split('/')[-1].split('_')[1])\n",
        "            # print(image_type + str(img_num))\n",
        "            print(img_num-1)\n",
        "\n",
        "            mask = Image.open(img_path)\n",
        "            mask = np.array(mask)\n",
        "            mask[mask != 0] = 1\n",
        "            save_path = os.path.join(test_labels, 'label_' + str(img_num-1) + '.jpg')\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(255*mask, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "\n",
        "ref_index = indexing-1\n",
        "for img_path in IMGS:\n",
        "    image_type = img_path.split('/')[-1].split('_')[0][:5]\n",
        "    if image_type == 'testB':\n",
        "        is_annotation = img_path.split('/')[-1].split('_')[-1].split('.bmp')[0]\n",
        "        if is_annotation != 'anno':\n",
        "            img_num = int(img_path.split('/')[-1].split('_')[1].split('.bmp')[0])\n",
        "            # print(image_type+str(img_num))\n",
        "            print(img_num + ref_index)\n",
        "\n",
        "            img = Image.open(img_path)\n",
        "            img = np.array(img)\n",
        "            save_path = os.path.join(test_images, 'image_' + str(img_num + ref_index) + '.jpg')\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "        else:\n",
        "            img_num = int(img_path.split('/')[-1].split('_')[1])\n",
        "            # print(image_type + str(img_num))\n",
        "            print(img_num+ref_index)\n",
        "\n",
        "            mask = Image.open(img_path)\n",
        "            mask = np.array(mask)\n",
        "            mask[mask != 0] = 1\n",
        "            save_path = os.path.join(test_labels, 'label_' + str(img_num+ref_index) + '.jpg')\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(255*mask, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "print('='*30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoDqD-n-5Cjx",
        "outputId": "8ec2cc25-f318-4689-9a7e-3b75fac325c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "0\n",
            "15\n",
            "13\n",
            "10\n",
            "10\n",
            "14\n",
            "12\n",
            "11\n",
            "9\n",
            "12\n",
            "15\n",
            "13\n",
            "9\n",
            "14\n",
            "16\n",
            "19\n",
            "18\n",
            "27\n",
            "26\n",
            "23\n",
            "24\n",
            "27\n",
            "21\n",
            "16\n",
            "24\n",
            "17\n",
            "20\n",
            "25\n",
            "18\n",
            "0\n",
            "20\n",
            "25\n",
            "22\n",
            "28\n",
            "17\n",
            "19\n",
            "26\n",
            "23\n",
            "1\n",
            "21\n",
            "22\n",
            "30\n",
            "33\n",
            "31\n",
            "39\n",
            "35\n",
            "34\n",
            "29\n",
            "37\n",
            "32\n",
            "31\n",
            "38\n",
            "40\n",
            "34\n",
            "39\n",
            "36\n",
            "2\n",
            "30\n",
            "38\n",
            "29\n",
            "33\n",
            "2\n",
            "40\n",
            "32\n",
            "35\n",
            "3\n",
            "37\n",
            "1\n",
            "36\n",
            "28\n",
            "46\n",
            "43\n",
            "41\n",
            "49\n",
            "48\n",
            "45\n",
            "52\n",
            "3\n",
            "47\n",
            "48\n",
            "50\n",
            "42\n",
            "44\n",
            "51\n",
            "41\n",
            "45\n",
            "46\n",
            "53\n",
            "52\n",
            "50\n",
            "43\n",
            "4\n",
            "49\n",
            "47\n",
            "44\n",
            "42\n",
            "51\n",
            "66\n",
            "4\n",
            "61\n",
            "64\n",
            "55\n",
            "65\n",
            "63\n",
            "53\n",
            "57\n",
            "54\n",
            "57\n",
            "62\n",
            "65\n",
            "60\n",
            "54\n",
            "60\n",
            "56\n",
            "58\n",
            "61\n",
            "59\n",
            "55\n",
            "58\n",
            "56\n",
            "5\n",
            "62\n",
            "63\n",
            "59\n",
            "64\n",
            "5\n",
            "69\n",
            "73\n",
            "68\n",
            "73\n",
            "75\n",
            "66\n",
            "71\n",
            "72\n",
            "74\n",
            "6\n",
            "77\n",
            "78\n",
            "76\n",
            "69\n",
            "67\n",
            "74\n",
            "77\n",
            "67\n",
            "78\n",
            "70\n",
            "68\n",
            "6\n",
            "72\n",
            "71\n",
            "70\n",
            "76\n",
            "75\n",
            "80\n",
            "84\n",
            "8\n",
            "79\n",
            "7\n",
            "82\n",
            "84\n",
            "81\n",
            "8\n",
            "81\n",
            "83\n",
            "79\n",
            "83\n",
            "80\n",
            "7\n",
            "82\n",
            "==============================\n",
            "11\n",
            "11\n",
            "10\n",
            "9\n",
            "0\n",
            "10\n",
            "9\n",
            "24\n",
            "1\n",
            "23\n",
            "16\n",
            "16\n",
            "12\n",
            "18\n",
            "12\n",
            "21\n",
            "20\n",
            "18\n",
            "17\n",
            "13\n",
            "25\n",
            "13\n",
            "20\n",
            "22\n",
            "23\n",
            "22\n",
            "14\n",
            "14\n",
            "17\n",
            "24\n",
            "19\n",
            "21\n",
            "15\n",
            "19\n",
            "15\n",
            "0\n",
            "29\n",
            "29\n",
            "27\n",
            "36\n",
            "28\n",
            "32\n",
            "38\n",
            "2\n",
            "1\n",
            "33\n",
            "33\n",
            "31\n",
            "34\n",
            "27\n",
            "25\n",
            "2\n",
            "37\n",
            "38\n",
            "26\n",
            "35\n",
            "34\n",
            "36\n",
            "26\n",
            "31\n",
            "35\n",
            "30\n",
            "28\n",
            "30\n",
            "37\n",
            "32\n",
            "49\n",
            "43\n",
            "50\n",
            "41\n",
            "46\n",
            "40\n",
            "47\n",
            "42\n",
            "39\n",
            "49\n",
            "4\n",
            "45\n",
            "44\n",
            "48\n",
            "3\n",
            "40\n",
            "41\n",
            "47\n",
            "42\n",
            "44\n",
            "3\n",
            "48\n",
            "45\n",
            "43\n",
            "39\n",
            "50\n",
            "46\n",
            "53\n",
            "8\n",
            "6\n",
            "52\n",
            "53\n",
            "57\n",
            "4\n",
            "7\n",
            "51\n",
            "52\n",
            "54\n",
            "58\n",
            "56\n",
            "5\n",
            "55\n",
            "59\n",
            "54\n",
            "56\n",
            "57\n",
            "51\n",
            "5\n",
            "7\n",
            "58\n",
            "8\n",
            "59\n",
            "55\n",
            "6\n",
            "78\n",
            "69\n",
            "60\n",
            "74\n",
            "61\n",
            "73\n",
            "77\n",
            "74\n",
            "70\n",
            "77\n",
            "71\n",
            "60\n",
            "75\n",
            "79\n",
            "62\n",
            "73\n",
            "75\n",
            "71\n",
            "72\n",
            "78\n",
            "76\n",
            "61\n",
            "79\n",
            "69\n",
            "70\n",
            "76\n",
            "72\n",
            "68\n",
            "63\n",
            "66\n",
            "64\n",
            "65\n",
            "64\n",
            "66\n",
            "65\n",
            "67\n",
            "68\n",
            "63\n",
            "67\n",
            "62\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to generate the unlabeled image patches for self-supervision from the official train set of GlaS\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "\n",
        "folder = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/train_images/*'\n",
        "\n",
        "# directories for images\n",
        "OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/unlabelled_img_patches'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "PATCHES = []\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split('.jpg')[0])):\n",
        "    image_number = int(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    ximg = transforms.ToTensor()(img)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = ximg.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(img.shape[2], -1, size, size)\n",
        "    patches = torch.permute(patches, (1, 2, 3, 0))\n",
        "    patches = patches.numpy()\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        save_path = os.path.join(OUT_FOLDER, 'image_' + str(indexing) + '.jpg')\n",
        "        cv2.imwrite(save_path, np.uint8(255*patches[i, :, :, :]))\n",
        "        indexing += 1\n",
        "    print('{} patches are created'.format(indexing))\n"
      ],
      "metadata": {
        "id": "AT5wS5Mo7Sxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9214315-918d-4c1f-e814-f81d79e63e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45 patches are created\n",
            "90 patches are created\n",
            "135 patches are created\n",
            "180 patches are created\n",
            "225 patches are created\n",
            "270 patches are created\n",
            "294 patches are created\n",
            "339 patches are created\n",
            "384 patches are created\n",
            "429 patches are created\n",
            "474 patches are created\n",
            "489 patches are created\n",
            "534 patches are created\n",
            "579 patches are created\n",
            "624 patches are created\n",
            "669 patches are created\n",
            "714 patches are created\n",
            "759 patches are created\n",
            "804 patches are created\n",
            "849 patches are created\n",
            "894 patches are created\n",
            "939 patches are created\n",
            "984 patches are created\n",
            "1029 patches are created\n",
            "1074 patches are created\n",
            "1119 patches are created\n",
            "1164 patches are created\n",
            "1209 patches are created\n",
            "1254 patches are created\n",
            "1299 patches are created\n",
            "1344 patches are created\n",
            "1389 patches are created\n",
            "1434 patches are created\n",
            "1479 patches are created\n",
            "1524 patches are created\n",
            "1548 patches are created\n",
            "1593 patches are created\n",
            "1638 patches are created\n",
            "1683 patches are created\n",
            "1728 patches are created\n",
            "1743 patches are created\n",
            "1788 patches are created\n",
            "1833 patches are created\n",
            "1878 patches are created\n",
            "1923 patches are created\n",
            "1968 patches are created\n",
            "2013 patches are created\n",
            "2058 patches are created\n",
            "2103 patches are created\n",
            "2127 patches are created\n",
            "2172 patches are created\n",
            "2217 patches are created\n",
            "2262 patches are created\n",
            "2307 patches are created\n",
            "2352 patches are created\n",
            "2397 patches are created\n",
            "2442 patches are created\n",
            "2487 patches are created\n",
            "2532 patches are created\n",
            "2577 patches are created\n",
            "2622 patches are created\n",
            "2667 patches are created\n",
            "2712 patches are created\n",
            "2757 patches are created\n",
            "2802 patches are created\n",
            "2847 patches are created\n",
            "2892 patches are created\n",
            "2937 patches are created\n",
            "2982 patches are created\n",
            "3027 patches are created\n",
            "3072 patches are created\n",
            "3117 patches are created\n",
            "3162 patches are created\n",
            "3207 patches are created\n",
            "3252 patches are created\n",
            "3297 patches are created\n",
            "3342 patches are created\n",
            "3387 patches are created\n",
            "3432 patches are created\n",
            "3477 patches are created\n",
            "3522 patches are created\n",
            "3567 patches are created\n",
            "3612 patches are created\n",
            "3636 patches are created\n",
            "3681 patches are created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to obtain the test image-label patches form the full-scale image-labels\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_images/*'\n",
        "folder2 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_masks/*'\n",
        "\n",
        "# directories for images and labels\n",
        "OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/images'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          IMAGE PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder1)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    ximg = transforms.ToTensor()(img)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = ximg.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(img.shape[2], -1, size, size)\n",
        "    patches = torch.permute(patches, (1, 2, 3, 0))\n",
        "    patches = patches.numpy()\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        save_path = os.path.join(OUT_FOLDER, 'image_' + str(indexing) + '.jpg')\n",
        "        cv2.imwrite(save_path, cv2.cvtColor(np.uint8(255 * patches[i, :, :, :]), cv2.COLOR_RGB2BGR))\n",
        "        indexing += 1\n",
        "print('Number of image patches in total: {}'.format(indexing))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          LABEL PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "\n",
        "LABEL_PATCHES = []\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder2)\n",
        "for label_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(label_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    mask = cv2.imread(label_path)\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    xlabel = transforms.ToTensor()(mask)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = xlabel.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(-1, size, size)\n",
        "    patches = np.uint8(patches.numpy())\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        LABEL_PATCHES.append(patches[i, :, :])\n",
        "        indexing += 1\n",
        "LABEL_PATCHES = np.array(LABEL_PATCHES)\n",
        "print('Number of label patches in total: {}'.format(LABEL_PATCHES.shape[0]))\n",
        "print(np.unique(LABEL_PATCHES))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                            Now Saving the patches                         ')\n",
        "print('===========================================================================')\n",
        "\n",
        "\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test', 'label_patches.mat')\n",
        "mdic = {\"data\": LABEL_PATCHES, \"label\": \"test_labels\"}\n",
        "savemat(path, mdic)\n",
        "print(f'Number of training labels: {LABEL_PATCHES.shape[0]}')\n"
      ],
      "metadata": {
        "id": "OU9SyfHY7cOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8580da14-b461-415c-f7d8-d0f6e61f293f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "                          IMAGE PATCHES                                    \n",
            "===========================================================================\n",
            "0\n",
            "1\n",
            "2\n",
            "Number of image patches in total: 135\n",
            "===========================================================================\n",
            "                          LABEL PATCHES                                    \n",
            "===========================================================================\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "Number of label patches in total: 435\n",
            "[0 1]\n",
            "===========================================================================\n",
            "                            Now Saving the patches                         \n",
            "===========================================================================\n",
            "Number of training labels: 435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to obtain the train image patches from the full scale train images\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/full_size_images/*'\n",
        "folder2 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/full_size_masks/*'\n",
        "\n",
        "# directories for images and labels\n",
        "OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/images'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          IMAGE PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder1)\n",
        "for img_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(img_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    ximg = transforms.ToTensor()(img)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = ximg.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(img.shape[2], -1, size, size)\n",
        "    patches = torch.permute(patches, (1, 2, 3, 0))\n",
        "    patches = patches.numpy()\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        save_path = os.path.join(OUT_FOLDER, 'image_' + str(indexing) + '.jpg')\n",
        "        cv2.imwrite(save_path, cv2.cvtColor(np.uint8(255 * patches[i, :, :, :]), cv2.COLOR_RGB2BGR))\n",
        "        indexing += 1\n",
        "print('Number of image patches in total: {}'.format(indexing))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          LABEL PATCHES                                    ')\n",
        "print('===========================================================================')\n",
        "\n",
        "LABEL_PATCHES = []\n",
        "indexing = 0\n",
        "IMGS = glob.glob(folder2)\n",
        "for label_path in sorted(IMGS, key=lambda x: int(x.split(\"_\")[-1].split(\".jpg\")[0])):\n",
        "    print(label_path.split(\"_\")[-1].split(\".jpg\")[0])\n",
        "    mask = cv2.imread(label_path)\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    xlabel = transforms.ToTensor()(mask)\n",
        "\n",
        "    size = 256  # patch size\n",
        "    stride = 64  # patch stride\n",
        "    patches = xlabel.unfold(1, size, stride).unfold(2, size, stride)\n",
        "    patches = patches.reshape(-1, size, size)\n",
        "    patches = np.uint8(patches.numpy())\n",
        "\n",
        "    for i in range(patches.shape[0]):\n",
        "        LABEL_PATCHES.append(patches[i, :, :])\n",
        "        indexing += 1\n",
        "LABEL_PATCHES = np.array(LABEL_PATCHES)\n",
        "print('Number of label patches in total: {}'.format(LABEL_PATCHES.shape[0]))\n",
        "print(np.unique(LABEL_PATCHES))\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                            Now Saving the patches                         ')\n",
        "print('===========================================================================')\n",
        "\n",
        "\n",
        "path = os.path.join('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train', 'label_patches.mat')\n",
        "mdic = {\"data\": LABEL_PATCHES, \"label\": \"train_labels\"}\n",
        "savemat(path, mdic)\n",
        "print(f'Number of training labels: {LABEL_PATCHES.shape[0]}')\n"
      ],
      "metadata": {
        "id": "4FUTveTs7gFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f6193d-7933-4a28-bd42-76e5c345050c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "                          IMAGE PATCHES                                    \n",
            "===========================================================================\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Number of image patches in total: 11\n",
            "===========================================================================\n",
            "                          LABEL PATCHES                                    \n",
            "===========================================================================\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Number of label patches in total: 1584\n",
            "[0]\n",
            "===========================================================================\n",
            "                            Now Saving the patches                         \n",
            "===========================================================================\n",
            "Number of training labels: 1584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This script is used to split the test set of the official GlaS into train-test split for our downstream\n",
        "# histopathological image segmentation\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "def split_data(images, labels, ratio):\n",
        "    idxs = np.random.RandomState(2023).permutation(images.shape[0])\n",
        "    split = int(images.shape[0] * ratio)\n",
        "    split_1 = idxs[:split]\n",
        "    split_2 = idxs[split:]\n",
        "    return images[split_1], images[split_2], labels[split_1], labels[split_2]\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/test_images'\n",
        "folder2 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/data/test_masks'\n",
        "\n",
        "# directories for images and labels\n",
        "TRAIN_OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/full_size_images'\n",
        "os.makedirs(TRAIN_OUT_FOLDER, exist_ok=True)\n",
        "TEST_OUT_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_images'\n",
        "os.makedirs(TEST_OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "TRAIN_LABEL_FOLDER = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/full_size_masks'\n",
        "os.makedirs(TRAIN_LABEL_FOLDER, exist_ok=True)\n",
        "TEST_LABEL_FOLDER = './content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/full_size_masks'\n",
        "os.makedirs(TEST_LABEL_FOLDER, exist_ok=True)\n",
        "\n",
        "print('===========================================================================')\n",
        "print('                          IMAGES and LABELS                                    ')\n",
        "print('===========================================================================')\n",
        "\n",
        "total_list1 = os.listdir(folder1)\n",
        "total_list1 = np.array(sorted(total_list1, key=lambda x: int(x.split('_')[-1].split('.jpg')[0])))\n",
        "\n",
        "total_list2 = os.listdir(folder2)\n",
        "total_list2 = np.array(sorted(total_list2, key=lambda x: int(x.split('_')[-1].split('.jpg')[0])))\n",
        "\n",
        "ratio = 0.8\n",
        "# Get the minimum length between the two lists\n",
        "\n",
        "min_len = min(len(total_list1), len(total_list2))\n",
        "\n",
        "# Generate indices based on the minimum length\n",
        "\n",
        "idxs = np.random.RandomState(2023).permutation(min_len)\n",
        "\n",
        "split = int(min_len * ratio)\n",
        "split_1 = idxs[:split]\n",
        "split_2 = idxs[split:]\n",
        "train_images, test_images = total_list1[split_1], total_list1[split_2]\n",
        "train_labels, test_labels = total_list2[split_1], total_list2[split_2]\n",
        "\n",
        "i = 0\n",
        "for img_name in train_images:\n",
        "    img = cv2.imread(os.path.join(folder1, img_name))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    TRAIN_OUT_IMAGE_PATH = os.path.join(TRAIN_OUT_FOLDER, 'image_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(TRAIN_OUT_IMAGE_PATH, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "    print(img_name)\n",
        "    i += 1\n",
        "\n",
        "print('='*30)\n",
        "\n",
        "i = 0\n",
        "for label_name in train_labels:\n",
        "    mask = cv2.imread(os.path.join(folder2, label_name))\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    TRAIN_OUT_IMAGE_PATH = os.path.join(TRAIN_LABEL_FOLDER, 'label_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(TRAIN_OUT_IMAGE_PATH, mask)\n",
        "    print(label_name)\n",
        "    i += 1\n",
        "\n",
        "print('='*30)\n",
        "\n",
        "i = 0\n",
        "for img_name in test_images:\n",
        "    img = cv2.imread(os.path.join(folder1, img_name))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    TEST_OUT_IMAGE_PATH = os.path.join(TEST_OUT_FOLDER, 'image_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(TEST_OUT_IMAGE_PATH, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "    print(img_name)\n",
        "    i += 1\n",
        "\n",
        "print('='*30)\n",
        "\n",
        "i = 0\n",
        "for label_name in test_labels:\n",
        "    mask = cv2.imread(os.path.join(folder2, label_name))\n",
        "    mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    TEST_OUT_IMAGE_PATH = os.path.join(TEST_LABEL_FOLDER, 'label_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(TEST_OUT_IMAGE_PATH, mask)\n",
        "    print(label_name)\n",
        "    i += 1\n"
      ],
      "metadata": {
        "id": "SQmw86AC7ZCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19bc2ea8-1cfd-48db-8eae-52eb698dca0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "                          IMAGES and LABELS                                    \n",
            "===========================================================================\n",
            "image_11.jpg\n",
            "image_8.jpg\n",
            "image_12.jpg\n",
            "image_2.jpg\n",
            "image_0.jpg\n",
            "image_5.jpg\n",
            "image_10.jpg\n",
            "image_4.jpg\n",
            "image_3.jpg\n",
            "image_1.jpg\n",
            "image_13.jpg\n",
            "==============================\n",
            "mask_11.jpg\n",
            "mask_8.jpg\n",
            "mask_12.jpg\n",
            "mask_2.jpg\n",
            "mask_0.jpg\n",
            "mask_5.jpg\n",
            "mask_10.jpg\n",
            "mask_4.jpg\n",
            "mask_3.jpg\n",
            "mask_1.jpg\n",
            "mask_13.jpg\n",
            "==============================\n",
            "image_6.jpg\n",
            "image_9.jpg\n",
            "image_7.jpg\n",
            "==============================\n",
            "mask_6.jpg\n",
            "mask_9.jpg\n",
            "mask_7.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import-ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo9a6t8RUBvx",
        "outputId": "f8e843f2-9db7-42a9-e3bc-21f013be74a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from import-ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from import-ipynb) (5.10.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (75.2.0)\n",
            "Collecting jedi>=0.16 (from IPython->import-ipynb)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->import-ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->import-ipynb) (5.7.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import-ipynb) (4.13.2)\n",
            "Downloading import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.2 jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import_ipynb\n",
        "import import_ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFO2napdVA3c",
        "outputId": "33ad9784-979e-40d2-8fff-d4249eae63ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.11/dist-packages (0.2)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (from import_ipynb) (7.34.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from import_ipynb) (5.10.4)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython->import_ipynb) (4.9.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->import_ipynb) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->import_ipynb) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->import_ipynb) (5.7.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.24.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.3.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->import_ipynb) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# Append the directory to your python path using sys\n",
        "sys.path.append('/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/pretrain/utils.ipynb')"
      ],
      "metadata": {
        "id": "3CkQ6eBdVR7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import Conv2d, ConvTranspose2d, Linear, Embedding\n",
        "from torch.nn import MaxPool2d, BatchNorm2d\n",
        "from torch.nn import LeakyReLU, Tanh, ReLU, Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.nn import MSELoss\n",
        "from torch import flatten\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import os, os.path\n",
        "from inspect import isfunction\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift=None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            # print(time_emb.shape)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = torch.einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim,\n",
        "            init_dim=None,\n",
        "            out_dim=None,\n",
        "            dim_mults=(1, 2, 4, 8),\n",
        "            channels=3,\n",
        "            with_time_emb=True,\n",
        "            resnet_block_groups=8,\n",
        "            convnext_mult=2,\n",
        "            encoder_only=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder_only = encoder_only\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # time embeddings\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # layers\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        if self.encoder_only is False:\n",
        "            print('decoder')\n",
        "            for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "                is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "                self.ups.append(\n",
        "                    nn.ModuleList(\n",
        "                        [\n",
        "                            block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                            block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                            Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                            Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                        ]\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            out_dim = default(out_dim, channels)\n",
        "            self.final_conv = nn.Sequential(\n",
        "                block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # down sample\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # bottleneck\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # up sample\n",
        "        if self.encoder_only is False:\n",
        "            for block1, block2, attn, upsample in self.ups:\n",
        "                x = torch.cat((x, h.pop()), dim=1)\n",
        "                x = block1(x, t)\n",
        "                x = block2(x, t)\n",
        "                x = attn(x)\n",
        "                x = upsample(x)\n",
        "\n",
        "            x = self.final_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiffusionNet(nn.Module):\n",
        "    def __init__(self, dim, channels):\n",
        "        super(DiffusionNet, self).__init__()\n",
        "\n",
        "        self.net = Unet(dim=dim, channels=channels, dim_mults=(1, 2, 4, 8))\n",
        "\n",
        "    def forward(self, x, time_stamps):\n",
        "\n",
        "        e = self.net(x, time_stamps)\n",
        "\n",
        "        return e\n"
      ],
      "metadata": {
        "id": "96P4MiTD7BAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10, CelebA\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from skimage import io\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import random\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "\n",
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\"\n",
        "    Returns a specific index t of a passed list of values vals\n",
        "    while considering the batch dimension.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "\n",
        "def forward_diffusion_sample(x_0, t, betas_schedule, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Takes an image and a timestep as input and\n",
        "    returns the noisy version of it\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(x_0)\n",
        "    sqrt_alphas_cumprod_t = get_index_from_list(betas_schedule['sqrt_alphas_cumprod'], t, x_0.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        betas_schedule['sqrt_one_minus_alphas_cumprod'], t, x_0.shape\n",
        "    )\n",
        "    # mean + variance\n",
        "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
        "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
        "\n",
        "\n",
        "def _my_normalization(x):\n",
        "    return (x * 2) - 1\n",
        "\n",
        "\n",
        "def get_images_list(path1, k=None):\n",
        "    total_list1 = os.listdir(path1)\n",
        "    total_list1 = sorted(total_list1, key=lambda x: int(x.split('_')[-1].split('.jpg')[0]))\n",
        "    if k is None:\n",
        "        return np.array(total_list1)\n",
        "    else:\n",
        "        return np.array(total_list1[:k])\n",
        "\n",
        "\n",
        "class Histo_Dataset(Dataset):\n",
        "    def __init__(self, image1_dir, image1_list, transform=None):\n",
        "        self.image1_dir = image1_dir\n",
        "        self.image1_list = image1_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img1_path = os.path.join(self.image1_dir, self.image1_list[index])\n",
        "\n",
        "        image1 = io.imread(img1_path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image1 = self.transform(image1)\n",
        "\n",
        "        return image1\n",
        "\n",
        "\n",
        "def load_transformed_dataset():\n",
        "    data_transforms = [\n",
        "        transforms.ToTensor(),  # Scales data into [0,1]\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.Lambda(_my_normalization)  # Scale between [-1, 1]\n",
        "    ]\n",
        "    data_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    data_size = None\n",
        "    TRAIN_IMAGE_DIR = './img_patches'  # Directory for the unlabeled images used for pretraining\n",
        "    img1_list = get_images_list(TRAIN_IMAGE_DIR, k=data_size)\n",
        "\n",
        "    ratio = 0.9\n",
        "    idxs = np.random.RandomState(2023).permutation(img1_list.shape[0])\n",
        "    split = int(img1_list.shape[0] * ratio)\n",
        "    train_index = idxs[:split]\n",
        "    valid_index = idxs[split:]\n",
        "\n",
        "    train_dataset = Histo_Dataset(TRAIN_IMAGE_DIR, img1_list[train_index], transform=data_transform)\n",
        "    eval_dataset = Histo_Dataset(TRAIN_IMAGE_DIR, img1_list[valid_index], transform=data_transform)\n",
        "\n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "\n",
        "def reverse_transforms_image(image):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: t.permute(1, 2, 0)),  # CHW to HWC\n",
        "        transforms.Lambda(lambda t: t * 255.),\n",
        "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "\n",
        "    # Take first image of batch\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0, :, :, :]\n",
        "    return reverse_transforms(image)\n",
        "\n",
        "\n",
        "def get_beta_schedule(betas):\n",
        "    schedule = {}\n",
        "    schedule['alphas'] = 1. - betas\n",
        "    schedule['alphas_cumprod'] = torch.cumprod(schedule['alphas'], dim=0)\n",
        "    schedule['alphas_cumprod_prev'] = F.pad(schedule['alphas_cumprod'][:-1], (1, 0), value=1.0)\n",
        "    schedule['sqrt_recip_alphas'] = torch.sqrt(1.0 / schedule['alphas'])\n",
        "    schedule['sqrt_alphas_cumprod'] = torch.sqrt(schedule['alphas_cumprod'])\n",
        "    schedule['sqrt_one_minus_alphas_cumprod'] = torch.sqrt(1. - schedule['alphas_cumprod'])\n",
        "    schedule['posterior_variance'] = betas * (1. - schedule['alphas_cumprod_prev']) / (\n",
        "                1. - schedule['alphas_cumprod'])\n",
        "    return schedule\n",
        "\n",
        "\n",
        "def get_loss(noise, noise_pred, time_stamps, betas_schedule, gpu):\n",
        "    t = time_stamps.cpu()\n",
        "    snr = 1.0 / (1 - betas_schedule['alphas_cumprod'][t]) - 1\n",
        "    k = 1.0\n",
        "    gamma = 1.0\n",
        "    lambda_t = 1.0/((k+snr)**gamma)\n",
        "    lambda_t = lambda_t.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(gpu)\n",
        "\n",
        "    n = noise.shape[1] * noise.shape[2] * noise.shape[3]\n",
        "    loss = torch.sum(lambda_t * F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "    return loss\n",
        "\n",
        "\n",
        "# def get_loss(noise, noise_pred, time_stamps, betas_schedule, gpu):\n",
        "#     t = time_stamps.cpu()\n",
        "#     n = noise.shape[1] * noise.shape[2] * noise.shape[3]\n",
        "\n",
        "#     snr = 1.0 / (1 - betas_schedule['alphas_cumprod'][t]) - 1\n",
        "#     k = 1.0\n",
        "#     gamma = 1.0\n",
        "#     lambda_t = 1.0/((k+snr)**gamma)\n",
        "#     lambda_t = lambda_t.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(gpu)\n",
        "#     loss1 = torch.sum(lambda_t * F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "\n",
        "#     scale_factor = (1.0 - betas_schedule['alphas'][t]) / (betas_schedule['alphas'][t] * (1.0 - betas_schedule['alphas_cumprod'][t]))\n",
        "#     scale_factor = scale_factor.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(gpu)\n",
        "#     loss2 = torch.sum(scale_factor * F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "\n",
        "#     c = 0.001\n",
        "#     loss = loss1 + c*loss2\n",
        "#     return loss\n",
        "\n",
        "\n",
        "# def get_loss(noise, noise_pred, time_stamps, betas_schedule, gpu):\n",
        "#     t = time_stamps.cpu()\n",
        "#     n = noise.shape[1] * noise.shape[2] * noise.shape[3]\n",
        "\n",
        "#     loss = torch.sum(F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "\n",
        "#     return loss\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, verbose=False, delta=0,\n",
        "                 path='checkpoint.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pth'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, ddp=False):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, ddp)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, ddp)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, epoch, ddp):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        if epoch != None:\n",
        "            weight_path = self.path[:-4] + '_' + str(epoch) + '_' + str(val_loss)[:7] + '.pth'\n",
        "        else:\n",
        "            weight_path = self.path\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'loss': val_loss,\n",
        "            'model_state_dict': model.module.state_dict(),\n",
        "        }, weight_path)\n",
        "\n",
        "        self.val_loss_min = val_loss\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eZVYPP7gJYG",
        "outputId": "6629ba7e-35e2-4726-cb6d-6cf7c865e9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-f0b218612b9d>:14: DeprecationWarning: Please import `map_coordinates` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.interpolation import map_coordinates\n",
            "<ipython-input-18-f0b218612b9d>:15: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/pretrain/utils.ipynb'\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/pretrain/model.ipynb'\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "#import utils\n",
        "#from model import DiffusionNet\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "from scipy.io import savemat\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_SIZE = 256\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0001\n",
        "T = 1000\n",
        "DATA_TYPE = 'diff_quadratic'\n",
        "\n",
        "betas = quadratic_beta_schedule(timesteps=T)\n",
        "betas_schedule = get_beta_schedule(betas)\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_timestep(model, x, t):\n",
        "    \"\"\"\n",
        "    Calls the model to predict the noise in the image and returns\n",
        "    the denoised image.\n",
        "    Applies noise to this image, if we are not in the last step yet.\n",
        "    \"\"\"\n",
        "    betas_t = get_index_from_list(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        betas_schedule['sqrt_one_minus_alphas_cumprod'], t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = get_index_from_list(betas_schedule['sqrt_recip_alphas'], t, x.shape)\n",
        "\n",
        "    # Call model (current image - noise prediction)\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "    posterior_variance_t = get_index_from_list(betas_schedule['posterior_variance'], t, x.shape)\n",
        "\n",
        "    if t == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_plot_image(model, gpu, epoch):\n",
        "    # Sample noise\n",
        "    img_size = IMG_SIZE\n",
        "    img = torch.randn((1, 3, img_size, img_size), device=gpu)\n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "    num_images = 100\n",
        "    stepsize = int(T/num_images)\n",
        "\n",
        "    all_images = []\n",
        "    for i in range(0,T)[::-1]:\n",
        "        t = torch.full((1,), i, device=gpu, dtype=torch.long)\n",
        "        img = sample_timestep(model, img, t)\n",
        "        if i % stepsize == 0:\n",
        "            all_images.append(img)\n",
        "\n",
        "    fig, axs = plt.subplots(10,10)\n",
        "    x=0\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            out_img = reverse_transforms_image(all_images[x].detach().cpu())\n",
        "            axs[i,j].imshow(out_img)\n",
        "            axs[i,j].axis('off')\n",
        "            x += 1\n",
        "    plt.savefig('./images/'+DATA_TYPE+'/image_' + str(epoch) + '.jpg', dpi=300)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the normal distribution\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
        "\n",
        "\n",
        "def train_epoch(train_dataloader, model, optimizer, gpu, epoch, args):\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    p_bar = tqdm(train_dataloader)\n",
        "\n",
        "    for img_batch in p_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        img_batch = img_batch.to(gpu, non_blocking=False)\n",
        "        t = torch.randint(0, T, (img_batch.shape[0],)).long()\n",
        "        t = t.to(gpu, non_blocking=False)\n",
        "        x_noisy, noise = forward_diffusion_sample(img_batch, t, betas_schedule, gpu)\n",
        "        noise_pred = model(x_noisy, t)\n",
        "        loss = get_loss(noise, noise_pred, t, betas_schedule, gpu)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        p_bar.set_description('Epoch {}'.format(epoch))\n",
        "        p_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print('Epoch: {}\\ttotal_loss {:.4f}'.format(epoch, np.mean(losses)))\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def eval_epoch(eval_dataloader, model, gpu, epoch, args, early_stopping=None):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        losses = []\n",
        "        p_bar = tqdm(eval_dataloader)\n",
        "\n",
        "        for img_batch in p_bar:\n",
        "            img_batch = img_batch.to(gpu, non_blocking=False)\n",
        "            t = torch.randint(0, T, (img_batch.shape[0],)).long()\n",
        "            t = t.to(gpu, non_blocking=False)\n",
        "            x_noisy, noise = forward_diffusion_sample(img_batch, t, betas_schedule, gpu)\n",
        "            noise_pred = model(x_noisy, t)\n",
        "            loss = get_loss(noise, noise_pred, t, betas_schedule, gpu)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            p_bar.set_description('Epoch {}'.format(epoch))\n",
        "            p_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print('Epoch: {}\\ttotal_loss {:.4f}'.format(epoch, np.mean(losses)))\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def main(gpu, args):\n",
        "    rank = args['nr'] * args['gpus'] + gpu\n",
        "    dist.init_process_group('nccl', rank=rank, world_size=args['world_size'])\n",
        "    torch.cuda.set_device(gpu)\n",
        "\n",
        "    data_size = None\n",
        "\n",
        "    # data loaders\n",
        "    train_dataset, eval_dataset = load_transformed_dataset()\n",
        "\n",
        "    train_sampler = torch.data.distributed.DistributedSampler(train_dataset,\n",
        "                                                                    num_replicas=args['world_size'],\n",
        "                                                                    rank=rank)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                drop_last=True, num_workers=4, pin_memory=True,\n",
        "                                sampler=train_sampler)\n",
        "\n",
        "    eval_sampler = torch.data.distributed.DistributedSampler(eval_dataset,\n",
        "                                                                    num_replicas=args['world_size'],\n",
        "                                                                    rank=rank, shuffle=False)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE,\n",
        "                                shuffle=False, drop_last=False, num_workers=4, pin_memory=True,\n",
        "                                sampler=eval_sampler)\n",
        "\n",
        "    model = DiffusionNet(dim=64, channels=3).to(gpu)\n",
        "    initialize_weights(model)\n",
        "    print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "    model = DDP(model, device_ids=[gpu], find_unused_parameters=True)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    checkpoint_path = args['checkpoints_path']\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    os.makedirs('./images/' + DATA_TYPE, exist_ok=True)\n",
        "\n",
        "    if args['load_from_chkpt'] is not None:\n",
        "        chkpt_file = args['load_from_chkpt']\n",
        "        print('Loading checkpoint from:', chkpt_file)\n",
        "        checkpoint = torch.load(chkpt_file)\n",
        "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    epoch_start = 0\n",
        "\n",
        "    if gpu == 0:\n",
        "        early_stopping = EarlyStopping(patience=15, verbose=True,\n",
        "                                             path=checkpoint_path + '{}_{}.pth'.format(BATCH_SIZE, LEARNING_RATE))\n",
        "    else:\n",
        "        early_stopping = None\n",
        "\n",
        "    train_losses = []\n",
        "    eval_losses = []\n",
        "    start_time = time.process_time()\n",
        "    for epoch in range(epoch_start, EPOCHS):\n",
        "        print('epoch {}/{}'.format(epoch + 1, EPOCHS))\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        train_loss = train_epoch(train_dataloader, model, optimizer, gpu, epoch + 1, args)\n",
        "        eval_loss = eval_epoch(eval_dataloader, model, gpu, epoch + 1, early_stopping)\n",
        "\n",
        "        mean_train_loss = torch.tensor(train_loss / args['gpus']).to(gpu)\n",
        "        mean_eval_loss = torch.tensor(eval_loss / args['gpus']).to(gpu)\n",
        "\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(mean_train_loss)\n",
        "        dist.all_reduce(mean_eval_loss)\n",
        "        print('gpu {} eval_loss:{}, mean_loss:{}'.format(gpu, eval_loss,\n",
        "                                                         mean_eval_loss.cpu().numpy()))\n",
        "\n",
        "        # if optim_name.split('-')[-1] == 'step':\n",
        "        #     scheduler.step(mean_eval_loss.cpu().numpy())\n",
        "        # elif optim_name.split('-')[-1] == 'cosine':\n",
        "        #     scheduler.step()\n",
        "        # elif optim_name.split('-')[-1] == 'no':\n",
        "        #     pass\n",
        "\n",
        "        if (epoch+1) % 20 == 0:\n",
        "            sample_plot_image(model, gpu, epoch+1)\n",
        "\n",
        "        if gpu == 0:\n",
        "            early_stopping(mean_eval_loss.cpu().numpy(), model, epoch + 1)\n",
        "\n",
        "        train_losses.append(mean_train_loss.cpu().numpy())\n",
        "        eval_losses.append(mean_eval_loss.cpu().numpy())\n",
        "\n",
        "    current_time = time.process_time()\n",
        "    print(\"Total Time Elapsed={:12.5} seconds\".format(str(current_time - start_time)))\n",
        "\n",
        "    # saving the plots\n",
        "    plots_path = './plots/diff'\n",
        "    os.makedirs(plots_path, exist_ok=True)\n",
        "    epochs = np.arange(EPOCHS)\n",
        "    train_losses = np.array(train_losses)\n",
        "    eval_losses = np.array(eval_losses)\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(8, 5))\n",
        "    axes.plot(epochs, train_losses, 'tab:blue', epochs, eval_losses, 'tab:orange')\n",
        "    axes.set_title(f'Training and Validation Loss (pretrained model = None, loss = MSE Loss, '\n",
        "                   f'data size = {data_size})',\n",
        "                   weight='bold', fontsize=7)\n",
        "    axes.set_xlabel('Epochs', weight='bold', fontsize=9)\n",
        "    axes.set_ylabel('Loss', weight='bold', fontsize=9)\n",
        "    plt.savefig(plots_path + '/'+DATA_TYPE+'loss_' + str(data_size) + '.jpg', dpi=300)\n",
        "\n",
        "    # os.makedirs('./loss', exist_ok=True)\n",
        "    # path = os.path.join('./loss', DATA_TYPE+'_train_loss.mat')\n",
        "    # mdic = {\"data\": train_losses, \"label\": \"epochs\"}\n",
        "    # savemat(path, mdic)\n",
        "\n",
        "    cleanup()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = {}\n",
        "\n",
        "    args['gpus'] = 1\n",
        "    args['nr'] = 0\n",
        "    args['world_size'] = args['gpus']\n",
        "    args['checkpoints_path'] =  './snapshots/' + DATA_TYPE + '/'\n",
        "    args['load_from_chkpt'] = None\n",
        "\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12345'\n",
        "\n",
        "    print(args['gpus'])\n",
        "   # mp.spawn(main, args=(args,), nprocs=args['gpus'])\n"
      ],
      "metadata": {
        "id": "BjUaw8cqIh3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14492b5a-c671-415e-abbb-b00eda4ac183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-59fbe827e85f>:14: DeprecationWarning: Please import `map_coordinates` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.interpolation import map_coordinates\n",
            "<ipython-input-19-59fbe827e85f>:15: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "\n",
        "class FLoss(nn.Module):\n",
        "    def __init__(self, gamma, weight=1.0):\n",
        "        super(FLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        '''\n",
        "            y_true shape: NxCxHxW\n",
        "            y_pred shape: NxCxHxW\n",
        "            '''\n",
        "\n",
        "        N = y_true.shape[0] * y_true.shape[2] * y_true.shape[3]\n",
        "        t2 = -self.weight * torch.pow((y_pred - 1), self.gamma) * y_true * torch.log(y_pred)\n",
        "        loss = torch.sum(t2) / N\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CELoss(nn.Module):\n",
        "    def __init__(self, weight=None):\n",
        "        super(CELoss, self).__init__()\n",
        "        self.weights = weight\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        '''\n",
        "                    y_true shape: NxCxHxW\n",
        "                    y_pred shape: NxCxHxW\n",
        "        '''\n",
        "\n",
        "        N = y_true.shape[0] * y_true.shape[2] * y_true.shape[3]\n",
        "        loss = -torch.sum(y_true * torch.log(y_pred)) / N\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SSLoss(nn.Module):\n",
        "    def __init__(self, beta=0.1, C=0.01):\n",
        "        super(SSLoss, self).__init__()\n",
        "        self.beta = beta\n",
        "        self.C = C\n",
        "        self.LCE = CELoss()\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        '''\n",
        "            y_true shape: NxCxHxW\n",
        "            y_pred shape: NxCxHxW\n",
        "        '''\n",
        "\n",
        "        mean_true = torch.mean(y_true, (2, 3), keepdim=True)\n",
        "        std_true = torch.std(y_true, (2, 3), keepdim=True)\n",
        "        mean_pred = torch.mean(y_pred, (2, 3), keepdim=True)\n",
        "        std_pred = torch.std(y_pred, (2, 3), keepdim=True)\n",
        "\n",
        "        e1 = (y_true - mean_true + self.C) / (std_true + self.C)\n",
        "        e2 = (y_pred - mean_pred + self.C) / (std_pred + self.C)\n",
        "        e = torch.abs(e1 - e2)\n",
        "\n",
        "        e_max, _ = torch.max(torch.flatten(e, start_dim=2), dim=2, keepdim=True)\n",
        "        e_max = torch.unsqueeze(e_max, dim=2)\n",
        "        f = (e > (self.beta * e_max)).float()\n",
        "\n",
        "        lce = self.LCE(y_pred, y_true)\n",
        "\n",
        "        loss = e * f * lce\n",
        "        M = torch.sum(f)\n",
        "\n",
        "        ssl_loss = torch.sum(loss)/M\n",
        "\n",
        "        return ssl_loss\n",
        "\n",
        "\n",
        "def tversky_coefficient(y_true, y_predict, smooth=1.0, beta=0.3):\n",
        "    intersection = torch.sum(y_true * y_predict)\n",
        "    i1 = beta * torch.sum((1-y_true) * y_predict)\n",
        "    i2 = (1-beta) * torch.sum(y_true * (1-y_predict))\n",
        "    return (intersection + smooth) / (intersection + i1 + i2 + smooth)\n",
        "\n",
        "\n",
        "class Tversky_Loss(nn.Module):\n",
        "    def __init__(self, beta):\n",
        "        super(Tversky_Loss, self).__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, y_predict, y_true):\n",
        "\n",
        "        tversky = 0.0\n",
        "        N = y_true.shape[0]\n",
        "        for i in range(y_true.shape[0]):\n",
        "            tversky += (1 - tversky_coefficient(y_true[i], y_predict[i], beta=self.beta))\n",
        "        loss = tversky/N\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class CosineLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CosineLoss, self).__init__()\n",
        "\n",
        "    def forward(self, y_predict, y_true):\n",
        "        N = y_true.shape[0] * y_true.shape[2] * y_true.shape[3]\n",
        "        product_sum = torch.sum(y_true * y_predict, dim=1)\n",
        "        loss = torch.sum(torch.cos((3.1416/2) * product_sum))/N\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class FocalLogLoss(nn.Module):\n",
        "    def __init__(self, gamma):\n",
        "        super(FocalLogLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, y_predict, y_true):\n",
        "        loss = torch.ones_like(y_true)\n",
        "        N = y_true.shape[0] * y_true.shape[1] * y_true.shape[2] * y_true.shape[3]\n",
        "        wrong_predictions = y_predict[y_true == 0]\n",
        "        loss[y_true == 0] = -15 * torch.pow(wrong_predictions, 2)\n",
        "        right_predictions = y_predict[y_true == 1]\n",
        "        loss[y_true != 0] = 15 * torch.pow((right_predictions - 1), self.gamma) * torch.log(right_predictions)\n",
        "\n",
        "        loss = -torch.sum(loss)/N\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LogMaxLoss(nn.Module):\n",
        "    def __init__(self, gamma):\n",
        "        super(LogMaxLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, y_predict, y_true):\n",
        "\n",
        "        y_false = 1.0 * torch.logical_not(y_true)\n",
        "        loss1 = 5 * torch.pow(torch.sum(y_false * y_predict, dim=1), 2)\n",
        "        loss2 = -5 * torch.sum(torch.pow((y_predict - 1), self.gamma) * y_true * torch.log(y_predict), dim=1)\n",
        "\n",
        "        log_max_loss = torch.mean(torch.maximum(loss1, loss2))\n",
        "\n",
        "        return log_max_loss\n",
        "\n",
        "\n",
        "class PolyLogLoss(nn.Module):\n",
        "    def __init__(self, gamma, weight=1.0):\n",
        "        super(PolyLogLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.weights = weight\n",
        "\n",
        "    def forward(self, y_predict, y_true):\n",
        "\n",
        "        right_predictions = torch.sum(y_true * y_predict, 1)\n",
        "        loss1 = -torch.pow((right_predictions - 1), self.gamma) * self.weights * torch.log(right_predictions)\n",
        "\n",
        "        y_false = 1.0 * torch.logical_not(y_true)\n",
        "        wrong_predictions = torch.sum(y_false * y_predict, 1)\n",
        "        loss2 = -torch.pow(wrong_predictions, self.gamma) * torch.log(wrong_predictions)\n",
        "\n",
        "        poly_log_loss = torch.mean(torch.abs(loss1 - loss2))\n",
        "\n",
        "        return poly_log_loss\n"
      ],
      "metadata": {
        "id": "YB7Buq2Iflba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "from scipy.io import loadmat, savemat\n",
        "from torchsummary import summary\n",
        "from torch.nn import functional as F\n",
        "from skimage import io\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision.models import resnet18, resnet50\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "from PIL import Image\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/downstream_train/losses.ipynb'\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/downstream_train/model.ipynb'\n",
        "#from losses import CELoss, Tversky_Loss, FLoss, SSLoss, PolyLogLoss\n",
        "#from model import SegNet\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# seed = 0  # You can choose any value as the seed\n",
        "# torch.manual_seed(seed)\n",
        "# torch.cuda.manual_seed(seed)\n",
        "# torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# Hyper Parameters\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "IMG_CHANNELS = 3\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 150\n",
        "NUM_CLASSES = 4 # number of classes\n",
        "TRAIN_IMAGE_DIR = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/images'\n",
        "TRAIN_LABEL_PATH = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/train/label_patches.mat'\n",
        "MODEL_TYPE = 'diff_quadratic_SSFL'\n",
        "LOSS_TYPE = 'SSFL' + '_' + str(BATCH_SIZE)\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "# Can use other image augmentations also\n",
        "def my_transforms(image1, mask):\n",
        "    if random.random() > 0.5:\n",
        "        image1 = TF.vflip(image1)\n",
        "        mask = TF.vflip(mask)\n",
        "\n",
        "    if random.random() > 0.5:\n",
        "        image1 = TF.hflip(image1)\n",
        "        mask = TF.hflip(mask)\n",
        "\n",
        "    if random.random() > 0.7:\n",
        "        image1 = TF.gaussian_blur(image1, [3, 3], [1.0, 2.0])\n",
        "\n",
        "    # if random.random() > 0.7:\n",
        "    #     image1 = TF.adjust_sharpness(image1, 2.0)\n",
        "\n",
        "    if random.random() > 0.7:\n",
        "        jitter = transforms.ColorJitter(brightness=.5, contrast=.4)\n",
        "        image1 = jitter(image1)\n",
        "\n",
        "    return image1, mask\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, verbose=False, delta=0,\n",
        "                 path='checkpoint.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pth'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, ddp=False):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, ddp)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, ddp)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, epoch, ddp):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        if epoch != None:\n",
        "            weight_path = self.path[:-4] + '_' + str(epoch) + '_' + str(val_loss)[:7] + '.pth'\n",
        "        else:\n",
        "            weight_path = self.path\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'loss': val_loss,\n",
        "            'model_state_dict': model.module.state_dict(),\n",
        "        }, weight_path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "def get_images_list(path1, k=None):\n",
        "    total_list1 = os.listdir(path1)\n",
        "    total_list1 = sorted(total_list1, key=lambda x: int(x.split('_')[-1].split('.jpg')[0]))\n",
        "\n",
        "    if k is None:\n",
        "        return np.array(total_list1)\n",
        "    else:\n",
        "        return np.array(total_list1[:k])\n",
        "\n",
        "\n",
        "class Histo_Dataset(Dataset):\n",
        "    def __init__(self, image1_dir, image1_list, label_list, transform=None):\n",
        "        self.image1_dir = image1_dir\n",
        "        self.image1_list = image1_list\n",
        "        self.label_list = label_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img1_path = os.path.join(self.image1_dir, self.image1_list[index])\n",
        "\n",
        "        image1 = io.imread(img1_path)\n",
        "        mask = self.label_list[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image1 = self.transform(image1)\n",
        "            mask = 255 * self.transform(mask)\n",
        "        image1, mask = my_transforms(image1, mask)\n",
        "\n",
        "        return image1, mask\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the normal distribution\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
        "\n",
        "\n",
        "def F1_score(y_true, y_pred):\n",
        "    class_f1_scores = []\n",
        "    _, y_true = torch.max(y_true, 0)\n",
        "    _, y_pred = torch.max(y_pred, 0)\n",
        "    for i in range(NUM_CLASSES):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = f1_score(true, pred, zero_division=1)\n",
        "        class_f1_scores.append(value)\n",
        "\n",
        "    return class_f1_scores\n",
        "\n",
        "\n",
        "def image_weights(y_true):\n",
        "    _, target_label = torch.max(y_true, dim=1)\n",
        "    img_weights = torch.zeros_like(y_true)\n",
        "    for i in range(y_true.shape[0]):\n",
        "        labels, label_counts = torch.unique(target_label[i], return_counts=True)\n",
        "        label_counts_avg = torch.mean(label_counts / torch.sum(label_counts))\n",
        "        img_weights[i] = 1.0 + label_counts_avg\n",
        "\n",
        "        return img_weights\n",
        "\n",
        "\n",
        "def class_weights(y_predict, y_true):\n",
        "    fscores = np.zeros(NUM_CLASSES)\n",
        "    for i in range(y_true.shape[0]):\n",
        "        fscores += F1_score(y_true[i], y_predict[i])\n",
        "    # fscores = fscores/y_true.shape[0]\n",
        "    # weights = (1 - fscores + 0.001) / (fscores + 0.001)\n",
        "\n",
        "    return fscores\n",
        "\n",
        "\n",
        "def train_epoch(train_loader, model, optimizer, gpu, epoch):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    p_bar = tqdm(train_loader)\n",
        "\n",
        "    for h, true_label in p_bar:\n",
        "        h = h.to(gpu, non_blocking=False)\n",
        "        true_label = true_label.squeeze(1).to(gpu, non_blocking=False)\n",
        "\n",
        "        target_label = F.one_hot(true_label.long(), NUM_CLASSES)\n",
        "        target_label = torch.permute(target_label, (0, 3, 1, 2))\n",
        "        target_label = target_label.float()\n",
        "\n",
        "        t = torch.full((h.shape[0],), 0, dtype=torch.long)\n",
        "        t = t.to(gpu, non_blocking=False)\n",
        "        predicted_label = model(h, t)\n",
        "\n",
        "        # ce_loss = CELoss()\n",
        "        ss_loss = SSLoss()\n",
        "        f_loss = FLoss(2.0)\n",
        "        # t_loss = Tversky_Loss(0.75)\n",
        "        # poly_log_loss = PolyLogLoss(2.0)\n",
        "\n",
        "        # loss1 = ce_loss(predicted_label, target_label)\n",
        "        loss2 = ss_loss(predicted_label, target_label)\n",
        "        loss3 = f_loss(predicted_label, target_label)\n",
        "        # loss4 = t_loss(predicted_label, target_label)\n",
        "        # loss5 = poly_log_loss(predicted_label, target_label)\n",
        "\n",
        "        loss = loss2 + loss3\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        p_bar.set_description('Epoch {}'.format(epoch))\n",
        "        p_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print('Epoch: {}\\ttotal_loss {:.4f}'.format(epoch, np.mean(losses)))\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def eval_epoch(eval_loader, model, gpu, epoch, early_stopping=None):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        val_loss = []\n",
        "        p_bar = tqdm(eval_loader)\n",
        "\n",
        "        total_items = 0.0\n",
        "        f_scores = np.zeros(NUM_CLASSES)\n",
        "        for h, true_label in p_bar:\n",
        "            h = h.to(gpu, non_blocking=False)\n",
        "            true_label = true_label.squeeze(1).to(gpu, non_blocking=False)\n",
        "\n",
        "            target_label = F.one_hot(true_label.long(), NUM_CLASSES)\n",
        "            target_label = torch.permute(target_label, (0, 3, 1, 2))\n",
        "            target_label = target_label.float()\n",
        "\n",
        "            t = torch.full((h.shape[0],), 0, dtype=torch.long)\n",
        "            t = t.to(gpu, non_blocking=False)\n",
        "\n",
        "            predicted_label = model(h, t)\n",
        "            f_scores += class_weights(predicted_label, target_label)\n",
        "            total_items += target_label.shape[0]\n",
        "\n",
        "            # ce_loss = CELoss()\n",
        "            ss_loss = SSLoss()\n",
        "            f_loss = FLoss(2.0)\n",
        "            # t_loss = Tversky_Loss(0.75)\n",
        "            # poly_log_loss = PolyLogLoss(2.0)\n",
        "\n",
        "            # loss1 = ce_loss(predicted_label, target_label)\n",
        "            loss2 = ss_loss(predicted_label, target_label)\n",
        "            loss3 = f_loss(predicted_label, target_label)\n",
        "            # loss4 = t_loss(predicted_label, target_label)\n",
        "            # loss5 = poly_log_loss(predicted_label, target_label)\n",
        "\n",
        "            loss = loss2 + loss3\n",
        "\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "            p_bar.set_description('Epoch {}'.format(epoch))\n",
        "            p_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    f_scores = f_scores / total_items\n",
        "    print(f_scores)\n",
        "    print(np.mean(f_scores[1:]))\n",
        "    # cls_weights = torch.ones((BATCH_SIZE, NUM_CLASSES, 256, 256)).to(device)\n",
        "    # for j in range(1, NUM_CLASSES):\n",
        "    #     cls_weights[:, j, :, :] = 10 * ((1 - f_scores[j] + 0.0001) / (f_scores[j] + 0.0001))\n",
        "\n",
        "    print('Epoch: {}\\tval_loss {:.4f}'.format(epoch, np.mean(val_loss)))\n",
        "    # early_stopping(np.mean(val_loss), model, epoch)\n",
        "\n",
        "    return np.mean(val_loss)\n",
        "\n",
        "\n",
        "def main(gpu, args):\n",
        "    rank = args['nr'] * args['gpus'] + gpu\n",
        "    dist.init_process_group('nccl', rank=rank, world_size=args['world_size'])\n",
        "    torch.cuda.set_device(gpu)\n",
        "\n",
        "    data_size = None\n",
        "    backbone = 'Unet'\n",
        "\n",
        "    # Loading the data\n",
        "    img1_list = get_images_list(TRAIN_IMAGE_DIR, k=data_size)\n",
        "    label_file = loadmat(TRAIN_LABEL_PATH)\n",
        "    train_labels = label_file['data']\n",
        "\n",
        "    ratio = 0.9\n",
        "    idxs = np.random.RandomState(2023).permutation(img1_list.shape[0])\n",
        "    split = int(img1_list.shape[0] * ratio)\n",
        "    train_index = idxs[:split]\n",
        "    valid_index = idxs[split:]\n",
        "\n",
        "    train_dataset = Histo_Dataset(TRAIN_IMAGE_DIR,\n",
        "                                  img1_list[train_index], train_labels[train_index],\n",
        "                                  transform=transformations)\n",
        "    eval_dataset = Histo_Dataset(TRAIN_IMAGE_DIR,\n",
        "                                 img1_list[valid_index], train_labels[valid_index],\n",
        "                                 transform=transformations)\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
        "                                                                    num_replicas=args['world_size'],\n",
        "                                                                    rank=rank)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                  drop_last=True, num_workers=4, pin_memory=True,\n",
        "                                  sampler=train_sampler)\n",
        "    eval_sampler = torch.utils.data.distributed.DistributedSampler(eval_dataset,\n",
        "                                                                   num_replicas=args['world_size'],\n",
        "                                                                   rank=rank, shuffle=False)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE,\n",
        "                                 shuffle=False, drop_last=False, num_workers=4, pin_memory=True,\n",
        "                                 sampler=eval_sampler)\n",
        "\n",
        "    model = SegNet(dim=64, channels=3, num_classes=4).to(gpu)\n",
        "    initialize_weights(model)\n",
        "\n",
        "    start_epoch = 0\n",
        "    if args['load_from_chkpt'] is not None:\n",
        "        chkpt_file = args['load_from_chkpt']\n",
        "        print('Loading checkpoint from:', chkpt_file)\n",
        "        checkpoint = torch.load(chkpt_file, map_location=torch.device('cpu'))\n",
        "        pretrained_dict = checkpoint['model_state_dict']\n",
        "        new_pretrained_dict = {k: v for k, v in pretrained_dict.items() if k[:15] != 'net.final_conv.'}\n",
        "        model.load_state_dict(new_pretrained_dict, strict=False)\n",
        "        # model.load_state_dict(pretrained_dict)\n",
        "\n",
        "    print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "    model = DDP(model, device_ids=[gpu], find_unused_parameters=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "    checkpoint_path = args['checkpoints_path']\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "\n",
        "    if gpu == 0:\n",
        "        early_stopping = EarlyStopping(patience=15, verbose=True,\n",
        "                                       path=checkpoint_path + '{}_{}.pth'.format(BATCH_SIZE, LEARNING_RATE))\n",
        "    else:\n",
        "        early_stopping = None\n",
        "\n",
        "    train_losses = []\n",
        "    eval_losses = []\n",
        "    start_time = time.process_time()\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        print('epoch {}/{}'.format(epoch + 1, EPOCHS))\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        train_loss = train_epoch(train_dataloader, model, optimizer, gpu, epoch + 1)\n",
        "        eval_loss = eval_epoch(eval_dataloader, model, gpu, epoch + 1, early_stopping)\n",
        "\n",
        "        mean_eval_loss = torch.tensor(eval_loss / args['gpus']).to(gpu)\n",
        "        mean_train_loss = torch.tensor(train_loss / args['gpus']).to(gpu)\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(mean_eval_loss)\n",
        "        dist.all_reduce(mean_train_loss)\n",
        "        print('gpu {} eval_loss:{}, mean_loss:{}'.format(gpu, eval_loss,\n",
        "                                                         mean_eval_loss.cpu().numpy()))\n",
        "\n",
        "        # if optim_name.split('-')[-1] == 'step':\n",
        "        #     scheduler.step(mean_eval_loss.cpu().numpy())\n",
        "        # elif optim_name.split('-')[-1] == 'cosine':\n",
        "        #     scheduler.step()\n",
        "        # elif optim_name.split('-')[-1] == 'no':\n",
        "        #     pass\n",
        "\n",
        "        if gpu == 0:\n",
        "            early_stopping(mean_eval_loss.cpu().numpy(), model, epoch + 1)\n",
        "        '''\n",
        "        if early_stopping.early_stop:\n",
        "            print('Early stop!')\n",
        "            break\n",
        "        '''\n",
        "        train_losses.append(mean_train_loss.cpu().numpy())\n",
        "        eval_losses.append(mean_eval_loss.cpu().numpy())\n",
        "\n",
        "    current_time = time.process_time()\n",
        "    print(\"Total Time Elapsed={:12.5} seconds\".format(str(current_time - start_time)))\n",
        "\n",
        "    # saving the plots\n",
        "    plots_path = './plots/' + MODEL_TYPE\n",
        "    os.makedirs(plots_path, exist_ok=True)\n",
        "    epochs = np.arange(start_epoch, EPOCHS)\n",
        "    train_losses = np.array(train_losses)\n",
        "    eval_losses = np.array(eval_losses)\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(8, 5))\n",
        "    axes.plot(epochs, train_losses, 'tab:blue', epochs, eval_losses, 'tab:orange')\n",
        "    axes.set_title(f'Training and Validation Loss (pretrained model = diffusion, loss = (SS + Focal) Loss, '\n",
        "                   f'data size = {data_size})',\n",
        "                   weight='bold', fontsize=7)\n",
        "    axes.set_xlabel('Epochs', weight='bold', fontsize=9)\n",
        "    axes.set_ylabel('Loss', weight='bold', fontsize=9)\n",
        "    axes.legend(['training loss', 'validation loss'], loc='best')\n",
        "    plt.savefig(plots_path + '/' + backbone + '_' + LOSS_TYPE + 'loss_' + str(data_size) + '.jpg', dpi=300)\n",
        "\n",
        "    cleanup()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = {}\n",
        "\n",
        "    args['gpus'] = 1\n",
        "    args['nr'] = 0\n",
        "    args['world_size'] = args['gpus']\n",
        "    args['checkpoints_path'] = './snapshots/' + MODEL_TYPE + '/'\n",
        "    args['load_from_chkpt'] = 'path for the diffusion pretrained checkpoint'\n",
        "\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12345'\n",
        "\n",
        "    print(args['gpus'])\n",
        "    #mp.spawn(main, args=(args,), nprocs=args['gpus'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTMkpDQCe2CJ",
        "outputId": "4b00a823-ae46-4304-b9c7-e2b0972722bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-22ea68a20cea>:21: DeprecationWarning: Please import `map_coordinates` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.interpolation import map_coordinates\n",
            "<ipython-input-21-22ea68a20cea>:22: DeprecationWarning: Please import `gaussian_filter` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import gaussian_filter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR10, CelebA\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from skimage import io\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "\n",
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "\n",
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
        "\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\"\n",
        "    Returns a specific index t of a passed list of values vals\n",
        "    while considering the batch dimension.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "\n",
        "def forward_diffusion_sample(x_0, t, betas_schedule, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Takes an image and a timestep as input and\n",
        "    returns the noisy version of it\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(x_0)\n",
        "    sqrt_alphas_cumprod_t = get_index_from_list(betas_schedule['sqrt_alphas_cumprod'], t, x_0.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        betas_schedule['sqrt_one_minus_alphas_cumprod'], t, x_0.shape\n",
        "    )\n",
        "    # mean + variance\n",
        "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
        "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
        "\n",
        "\n",
        "def _my_normalization(x):\n",
        "    return (x * 2) - 1\n",
        "\n",
        "\n",
        "def get_images_list(folder1, folder2, folder3, k=None):\n",
        "    total_list1 = os.listdir(folder1)\n",
        "    total_list1 = sorted(total_list1, key=lambda x: int(x.split('_')[-1].split('.jpg')[0]))\n",
        "    path1_list = [os.path.join(folder1, f) for f in total_list1]\n",
        "\n",
        "    total_list2 = os.listdir(folder2)\n",
        "    total_list2 = sorted(total_list2, key=lambda x: int(x.split('_')[-1].split('.jpg')[0]))\n",
        "    path2_list = [os.path.join(folder2, f) for f in total_list2]\n",
        "\n",
        "    total_list3 = os.listdir(folder3)\n",
        "    total_list3 = sorted(total_list3, key=lambda x: int(x.split('_')[-1].split('.jpg')[0]))\n",
        "    path3_list = [os.path.join(folder3, f) for f in total_list3]\n",
        "    if k is None:\n",
        "        return path1_list, path2_list, path3_list\n",
        "    else:\n",
        "        return path1_list[:k], path2_list[:k], path3_list[:k]\n",
        "\n",
        "\n",
        "class AIIMS_Dataset(Dataset):\n",
        "    def __init__(self, images_list, transform=None):\n",
        "        self.transform = transform\n",
        "        self.images_list = images_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.images_list[index]\n",
        "        image = io.imread(img_path)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "\n",
        "def load_transformed_dataset():\n",
        "    data_transforms = [\n",
        "        transforms.ToTensor(), # Scales data into [0,1]\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.Lambda(_my_normalization) # Scale between [-1, 1]\n",
        "    ]\n",
        "    data_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    data_size = None\n",
        "    TRAIN_IMAGE_DIR1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/pre_process/HN/unlabelled_img_patches'\n",
        "    TRAIN_IMAGE_DIR2 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/pre_process/MoNuSeg/unlabelled_img_patches'\n",
        "    TRAIN_IMAGE_DIR3 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/pre_process/GlaS/unlabelled_img_patches'\n",
        "    img1_list, img2_list, img3_list = get_images_list(TRAIN_IMAGE_DIR1, TRAIN_IMAGE_DIR2, TRAIN_IMAGE_DIR3, k=data_size)\n",
        "    img_list = np.array(img1_list + img2_list + img3_list)\n",
        "\n",
        "    ratio = 0.9\n",
        "    idxs = np.random.RandomState(2023).permutation(img_list.shape[0])\n",
        "    split = int(img_list.shape[0] * ratio)\n",
        "    train_index = idxs[:split]\n",
        "    valid_index = idxs[split:]\n",
        "\n",
        "    train_dataset = AIIMS_Dataset(img_list[train_index], transform=data_transform)\n",
        "    eval_dataset = AIIMS_Dataset(img_list[valid_index], transform=data_transform)\n",
        "\n",
        "\n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "\n",
        "def reverse_transforms_image(image):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "        transforms.Lambda(lambda t: t * 255.),\n",
        "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "\n",
        "    # Take first image of batch\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0, :, :, :]\n",
        "    return reverse_transforms(image)\n",
        "\n",
        "def get_beta_schedule(betas):\n",
        "    schedule = {}\n",
        "    schedule['alphas'] = 1. - betas\n",
        "    schedule['alphas_cumprod'] = torch.cumprod(schedule['alphas'], dim=0)\n",
        "    schedule['alphas_cumprod_prev'] = F.pad(schedule['alphas_cumprod'][:-1], (1, 0), value=1.0)\n",
        "    schedule['sqrt_recip_alphas'] = torch.sqrt(1.0 / schedule['alphas'])\n",
        "    schedule['sqrt_alphas_cumprod'] = torch.sqrt(schedule['alphas_cumprod'])\n",
        "    schedule['sqrt_one_minus_alphas_cumprod'] = torch.sqrt(1. - schedule['alphas_cumprod'])\n",
        "    schedule['posterior_variance'] = betas * (1. - schedule['alphas_cumprod_prev']) / (\n",
        "                1. - schedule['alphas_cumprod'])\n",
        "    return schedule\n",
        "\n",
        "\n",
        "def get_loss(noise, noise_pred, time_stamps, betas_schedule, gpu):\n",
        "    t = time_stamps.cpu()\n",
        "    snr = 1.0 / (1 - betas_schedule['alphas_cumprod'][t]) - 1\n",
        "    k = 1.0\n",
        "    gamma = 1.0\n",
        "    lambda_t = 1.0/((k+snr)**gamma)\n",
        "    lambda_t = lambda_t.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(gpu)\n",
        "\n",
        "    n = noise.shape[1] * noise.shape[2] * noise.shape[3]\n",
        "    loss = torch.sum(lambda_t * F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "    return loss\n",
        "\n",
        "\n",
        "# def get_loss(noise, noise_pred, time_stamps, betas_schedule, gpu):\n",
        "#     t = time_stamps.cpu()\n",
        "#     n = noise.shape[1] * noise.shape[2] * noise.shape[3]\n",
        "\n",
        "#     snr = 1.0 / (1 - betas_schedule['alphas_cumprod'][t]) - 1\n",
        "#     k = 1.0\n",
        "#     gamma = 1.0\n",
        "#     lambda_t = 1.0/((k+snr)**gamma)\n",
        "#     lambda_t = lambda_t.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(gpu)\n",
        "#     loss1 = torch.sum(lambda_t * F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "\n",
        "#     scale_factor = (1.0 - betas_schedule['alphas'][t]) / (betas_schedule['alphas'][t] * (1.0 - betas_schedule['alphas_cumprod'][t]))\n",
        "#     scale_factor = scale_factor.unsqueeze(1).unsqueeze(2).unsqueeze(3).to(gpu)\n",
        "#     loss2 = torch.sum(scale_factor * F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "\n",
        "#     c = 0.001\n",
        "#     loss = loss1 + c*loss2\n",
        "#     return loss\n",
        "\n",
        "\n",
        "# def get_loss(noise, noise_pred, time_stamps, betas_schedule, gpu):\n",
        "#     t = time_stamps.cpu()\n",
        "#     n = noise.shape[1] * noise.shape[2] * noise.shape[3]\n",
        "\n",
        "#     loss = torch.sum(F.mse_loss(noise, noise_pred, reduction='none'))/n\n",
        "\n",
        "#     return loss\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=10, verbose=False, delta=0,\n",
        "                 path='checkpoint.pth'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pth'\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model, epoch=None, ddp=False):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, ddp)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, epoch, ddp)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, epoch, ddp):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        if epoch != None:\n",
        "            weight_path = self.path[:-4] + '_' + str(epoch) + '_' + str(val_loss)[:7] + '.pth'\n",
        "        else:\n",
        "            weight_path = self.path\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'loss': val_loss,\n",
        "            'model_state_dict': model.module.state_dict(),\n",
        "        }, weight_path)\n",
        "\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "gam59G9DmVCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/utils_general.ipynb'\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/downstream_train/model.ipynb'\n",
        "#import utils_general as utils\n",
        "#from GenSelfDiff.downstream_train.model import DiffusionNet\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "from scipy.io import savemat\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_SIZE = 256\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 1e-4\n",
        "T = 1000\n",
        "DATA_TYPE = 'diff_quadratic_general'\n",
        "\n",
        "betas = quadratic_beta_schedule(timesteps=T)\n",
        "betas_schedule = get_beta_schedule(betas)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_timestep(model, x, t):\n",
        "    \"\"\"\n",
        "    Calls the model to predict the noise in the image and returns\n",
        "    the denoised image.\n",
        "    Applies noise to this image, if we are not in the last step yet.\n",
        "    \"\"\"\n",
        "    betas_t = get_index_from_list(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        betas_schedule['sqrt_one_minus_alphas_cumprod'], t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = get_index_from_list(betas_schedule['sqrt_recip_alphas'], t, x.shape)\n",
        "\n",
        "    # Call model (current image - noise prediction)\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "    posterior_variance_t = get_index_from_list(betas_schedule['posterior_variance'], t, x.shape)\n",
        "\n",
        "    if t == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_plot_image(model, gpu, epoch):\n",
        "    # Sample noise\n",
        "    img_size = IMG_SIZE\n",
        "    img = torch.randn((1, 3, img_size, img_size), device=gpu)\n",
        "    plt.figure()\n",
        "    plt.axis('off')\n",
        "    num_images = 100\n",
        "    stepsize = int(T/num_images)\n",
        "\n",
        "    all_images = []\n",
        "    for i in range(0,T)[::-1]:\n",
        "        t = torch.full((1,), i, device=gpu, dtype=torch.long)\n",
        "        img = sample_timestep(model, img, t)\n",
        "        if i % stepsize == 0:\n",
        "            all_images.append(img)\n",
        "\n",
        "    fig, axs = plt.subplots(10,10)\n",
        "    x=0\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            out_img = reverse_transforms_image(all_images[x].detach().cpu())\n",
        "            axs[i,j].imshow(out_img)\n",
        "            axs[i,j].axis('off')\n",
        "            x += 1\n",
        "    plt.savefig('./images/'+DATA_TYPE+'/image_' + str(epoch) + '.jpg', dpi=300)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the normal distribution\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d, nn.GroupNorm)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.01)\n",
        "\n",
        "def train_epoch(train_dataloader, model, optimizer, gpu, epoch, args):\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    p_bar = tqdm(train_dataloader)\n",
        "\n",
        "    for img_batch in p_bar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        img_batch = img_batch.to(gpu, non_blocking=False)\n",
        "        t = torch.randint(0, T, (img_batch.shape[0],)).long()\n",
        "        t = t.to(gpu, non_blocking=False)\n",
        "        x_noisy, noise = forward_diffusion_sample(img_batch, t, betas_schedule, gpu)\n",
        "        noise_pred = model(x_noisy, t)\n",
        "        loss = get_loss(noise, noise_pred, t, betas_schedule, gpu)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        p_bar.set_description('Epoch {}'.format(epoch))\n",
        "        p_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print('Epoch: {}\\ttotal_loss {:.4f}'.format(epoch, np.mean(losses)))\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def eval_epoch(eval_dataloader, model, gpu, epoch, args, early_stopping=None):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        losses = []\n",
        "        p_bar = tqdm(eval_dataloader)\n",
        "\n",
        "        for img_batch in p_bar:\n",
        "            img_batch = img_batch.to(gpu, non_blocking=False)\n",
        "            t = torch.randint(0, T, (img_batch.shape[0],)).long()\n",
        "            t = t.to(gpu, non_blocking=False)\n",
        "            x_noisy, noise = forward_diffusion_sample(img_batch, t, betas_schedule, gpu)\n",
        "            noise_pred = model(x_noisy, t)\n",
        "            loss = get_loss(noise, noise_pred, t, betas_schedule, gpu)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            p_bar.set_description('Epoch {}'.format(epoch))\n",
        "            p_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print('Epoch: {}\\ttotal_loss {:.4f}'.format(epoch, np.mean(losses)))\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "\n",
        "def main(gpu, args):\n",
        "    rank = args['nr'] * args['gpus'] + gpu\n",
        "    dist.init_process_group('nccl', rank=rank, world_size=args['world_size'])\n",
        "    torch.cuda.set_device(gpu)\n",
        "\n",
        "    data_size = None\n",
        "    # img1_list = get_images_list(TRAIN_IMAGE_DIR, k=data_size)\n",
        "\n",
        "    # data loaders\n",
        "    train_dataset, eval_dataset = load_transformed_dataset()\n",
        "\n",
        "    train_sampler = torch.data.distributed.DistributedSampler(train_dataset,\n",
        "                                                                    num_replicas=args['world_size'],\n",
        "                                                                    rank=rank)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                drop_last=True, num_workers=4, pin_memory=True,\n",
        "                                sampler=train_sampler)\n",
        "\n",
        "    eval_sampler = torch.data.distributed.DistributedSampler(eval_dataset,\n",
        "                                                                    num_replicas=args['world_size'],\n",
        "                                                                    rank=rank, shuffle=False)\n",
        "    eval_dataloader = DataLoader(eval_dataset, batch_size=BATCH_SIZE,\n",
        "                                shuffle=False, drop_last=False, num_workers=4, pin_memory=True,\n",
        "                                sampler=eval_sampler)\n",
        "\n",
        "\n",
        "    model = DiffusionNet(dim=64, channels=3).to(gpu)\n",
        "    initialize_weights(model)\n",
        "    print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "    model = DDP(model, device_ids=[gpu], find_unused_parameters=True)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    checkpoint_path = args['checkpoints_path']\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    os.makedirs('./images/' + DATA_TYPE, exist_ok=True)\n",
        "\n",
        "    epoch_start = 0\n",
        "    if args['load_from_chkpt'] is not None:\n",
        "        chkpt_file = args['load_from_chkpt']\n",
        "        print('Loading checkpoint from:', chkpt_file)\n",
        "        checkpoint = torch.load(chkpt_file)\n",
        "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if gpu == 0:\n",
        "        early_stopping = EarlyStopping(patience=15, verbose=True,\n",
        "                                             path=checkpoint_path + '{}_{}.pth'.format(BATCH_SIZE, LEARNING_RATE))\n",
        "    else:\n",
        "        early_stopping = None\n",
        "\n",
        "    train_losses = []\n",
        "    eval_losses = []\n",
        "    start_time = time.process_time()\n",
        "    for epoch in range(epoch_start, EPOCHS):\n",
        "        print('epoch {}/{}'.format(epoch + 1, EPOCHS))\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        train_loss = train_epoch(train_dataloader, model, optimizer, gpu, epoch + 1, args)\n",
        "        eval_loss = eval_epoch(eval_dataloader, model, gpu, epoch + 1, early_stopping)\n",
        "\n",
        "        mean_train_loss = torch.tensor(train_loss / args['gpus']).to(gpu)\n",
        "        mean_eval_loss = torch.tensor(eval_loss / args['gpus']).to(gpu)\n",
        "\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(mean_train_loss)\n",
        "        dist.all_reduce(mean_eval_loss)\n",
        "        print('gpu {} eval_loss:{}, mean_loss:{}'.format(gpu, eval_loss,\n",
        "                                                         mean_eval_loss.cpu().numpy()))\n",
        "\n",
        "        # if optim_name.split('-')[-1] == 'step':\n",
        "        #     scheduler.step(mean_eval_loss.cpu().numpy())\n",
        "        # elif optim_name.split('-')[-1] == 'cosine':\n",
        "        #     scheduler.step()\n",
        "        # elif optim_name.split('-')[-1] == 'no':\n",
        "        #     pass\n",
        "\n",
        "        # if (epoch+1) % 10 == 0:\n",
        "        #     sample_plot_image(model, gpu, epoch+1)\n",
        "\n",
        "        if gpu == 0:\n",
        "            early_stopping(mean_eval_loss.cpu().numpy(), model, epoch + 1)\n",
        "\n",
        "        train_losses.append(mean_train_loss.cpu().numpy())\n",
        "        eval_losses.append(mean_eval_loss.cpu().numpy())\n",
        "\n",
        "    current_time = time.process_time()\n",
        "    print(\"Total Time Elapsed={:12.5} seconds\".format(str(current_time - start_time)))\n",
        "\n",
        "    # saving the plots\n",
        "    plots_path = './plots/diff'\n",
        "    os.makedirs(plots_path, exist_ok=True)\n",
        "    epochs = np.arange(epoch_start, EPOCHS)\n",
        "    train_losses = np.array(train_losses)\n",
        "    eval_losses = np.array(eval_losses)\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(8, 5))\n",
        "    axes.plot(epochs, train_losses, 'tab:blue', epochs, eval_losses, 'tab:orange')\n",
        "    axes.set_title(f'Training and Validation Loss (pretrained model = None, loss = MSE Loss, '\n",
        "                   f'data size = {data_size})',\n",
        "                   weight='bold', fontsize=7)\n",
        "    axes.set_xlabel('Epochs', weight='bold', fontsize=9)\n",
        "    axes.set_ylabel('Loss', weight='bold', fontsize=9)\n",
        "    plt.savefig(plots_path + '/'+DATA_TYPE+'loss_' + str(data_size) + '.jpg', dpi=300)\n",
        "\n",
        "    # os.makedirs('./loss', exist_ok=True)\n",
        "    # path = os.path.join('./loss', DATA_TYPE+'_train_loss.mat')\n",
        "    # mdic = {\"data\": train_losses, \"label\": \"epochs\"}\n",
        "    # savemat(path, mdic)\n",
        "\n",
        "    cleanup()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = {}\n",
        "\n",
        "    args['gpus'] = 4\n",
        "    args['nr'] = 0\n",
        "    args['world_size'] = args['gpus']\n",
        "    args['checkpoints_path'] =  './snapshots/' + DATA_TYPE + '/'\n",
        "    args['load_from_chkpt'] = None\n",
        "\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12345'\n",
        "\n",
        "    print(args['gpus'])\n",
        "    #mp.spawn(main, args=(args,), nprocs=args['gpus'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOQTVhIzll25",
        "outputId": "9da3b41f-9485-4007-ba0b-c641f0e0bb80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, jaccard_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.spatial.distance import directed_hausdorff\n",
        "\n",
        "start = 1 # This is set to 0 if class:0 (background in our case) is to included as one class in the metric computation, Otherwise it is set to 1\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    intersection = np.sum(y_true * y_pred)\n",
        "    smooth = 1.0\n",
        "    return ((2. * intersection) + smooth) / (np.sum(y_true) + np.sum(y_pred) + smooth)\n",
        "\n",
        "\n",
        "def dice_score(y_true, y_pred, num_classes):\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    dice = []\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        dice.append(dice_coef(true, pred))\n",
        "\n",
        "    return dice\n",
        "\n",
        "\n",
        "def conf_matrix(y_true, y_pred, num_classes):\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "    cm = confusion_matrix(y_true.reshape(-1), y_pred.reshape(-1), labels=np.arange(num_classes))\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred, num_classes):\n",
        "    class_precisions = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = precision_score(true, pred, zero_division=1)  # * y_true.shape[0]\n",
        "        class_precisions.append(value)\n",
        "\n",
        "    return class_precisions\n",
        "\n",
        "\n",
        "def sensitivity(y_true, y_pred, num_classes):\n",
        "    class_sensitivities = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = recall_score(true, pred, zero_division=1)  # * y_true.shape[0]\n",
        "        class_sensitivities.append(value)\n",
        "\n",
        "    return class_sensitivities\n",
        "\n",
        "\n",
        "def specificity(y_true, y_pred, num_classes):\n",
        "    class_specifities = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = recall_score(true, pred, pos_label=0, zero_division=1)  # * y_true.shape[0]\n",
        "        class_specifities.append(value)\n",
        "\n",
        "    return class_specifities\n",
        "\n",
        "\n",
        "def accuracy(y_true, y_pred, num_classes):\n",
        "    class_accuracies = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = accuracy_score(true, pred)  # * y_true.shape[0]\n",
        "        class_accuracies.append(value)\n",
        "\n",
        "    return class_accuracies\n",
        "\n",
        "\n",
        "def F1_score(y_true, y_pred, num_classes):\n",
        "    class_f1_scores = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = f1_score(true, pred, zero_division=1)  # * y_true.shape[0]\n",
        "        class_f1_scores.append(value)\n",
        "\n",
        "    return class_f1_scores\n",
        "\n",
        "\n",
        "def Jaccard_score(y_true, y_pred, num_classes):\n",
        "    class_jaccard_scores = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).reshape(-1).cpu().numpy()\n",
        "        pred = (y_pred == i).reshape(-1).cpu().numpy()\n",
        "        value = jaccard_score(true, pred, zero_division=1)  # * y_true.shape[0]\n",
        "        class_jaccard_scores.append(value)\n",
        "\n",
        "    return class_jaccard_scores\n",
        "\n",
        "\n",
        "def Hausdorff_distance(y_true, y_pred, num_classes):\n",
        "    class_hd = []\n",
        "    _, y_true = torch.max(y_true, 1)\n",
        "    _, y_pred = torch.max(y_pred, 1)\n",
        "    for i in range(start, num_classes):\n",
        "        true = (y_true == i).squeeze(0).cpu().numpy()\n",
        "        pred = (y_pred == i).squeeze(0).cpu().numpy()\n",
        "        hd1 = directed_hausdorff(true, pred)[0]\n",
        "        hd2 = directed_hausdorff(pred, true)[0]\n",
        "        hd = max(hd1, hd2)\n",
        "        class_hd.append(hd)\n",
        "\n",
        "    return class_hd\n",
        "\n",
        "# This script is adopted from the offical repository of AJI score\n",
        "def Aggregated_jaccard_index(gt_map, predicted_map, gpu):\n",
        "    _, gt_map = torch.max(gt_map, 1)\n",
        "    _, predicted_map = torch.max(predicted_map, 1)\n",
        "\n",
        "    gt_list = torch.unique(gt_map)\n",
        "    pr_list = torch.unique(predicted_map)\n",
        "\n",
        "    if start != 0:\n",
        "        gt_list = gt_list[gt_list != 0]\n",
        "        pr_list = pr_list[pr_list != 0]\n",
        "\n",
        "    pr_list = torch.cat((pr_list.view(-1, 1), torch.zeros(pr_list.size(0), 1).to(gpu)), dim=1)\n",
        "\n",
        "    overall_correct_count = 0.0\n",
        "    union_pixel_count = 0.0\n",
        "\n",
        "    i = len(gt_list)\n",
        "\n",
        "    while len(gt_list) > 0:\n",
        "        # print(f'Processing object # {i}')\n",
        "\n",
        "        gt = (gt_map == gt_list[i - 1]).float()\n",
        "\n",
        "        predicted_match = gt * predicted_map.float()\n",
        "\n",
        "        if predicted_match.sum() == 0:\n",
        "            union_pixel_count += gt.sum()\n",
        "            gt_list = gt_list[:-1]\n",
        "            i = len(gt_list)\n",
        "        else:\n",
        "            predicted_nuc_index = torch.unique(predicted_match)\n",
        "            if start != 0:\n",
        "                predicted_nuc_index = predicted_nuc_index[predicted_nuc_index != 0]\n",
        "\n",
        "            JI = 0\n",
        "            best_match = None\n",
        "\n",
        "            for j in range(len(predicted_nuc_index)):\n",
        "                matched = (predicted_map == predicted_nuc_index[j]).float()\n",
        "                nJI = matched.logical_and(gt).sum() / matched.logical_or(gt).sum()\n",
        "\n",
        "                if nJI > JI:\n",
        "                    best_match = predicted_nuc_index[j]\n",
        "                    JI = nJI\n",
        "\n",
        "            predicted_nuclei = (predicted_map == best_match).float()\n",
        "\n",
        "            overall_correct_count += (gt.logical_and(predicted_nuclei)).sum()\n",
        "            union_pixel_count += (gt.logical_or(predicted_nuclei)).sum()\n",
        "\n",
        "            gt_list = gt_list[:-1]\n",
        "            i = len(gt_list)\n",
        "\n",
        "            best_match_idx = (pr_list[:, 0] == best_match).nonzero().item()\n",
        "            pr_list[best_match_idx, 1] += 1\n",
        "\n",
        "    unused_nuclei_list = (pr_list[:, 1] == 0).nonzero().view(-1)\n",
        "\n",
        "    for k in range(len(unused_nuclei_list)):\n",
        "        print(pr_list[unused_nuclei_list[k], 0])\n",
        "        unused_nuclei = (predicted_map == pr_list[unused_nuclei_list[k], 0]).float()\n",
        "        union_pixel_count += unused_nuclei.sum()\n",
        "\n",
        "    if overall_correct_count == 0 and union_pixel_count == 0:\n",
        "        return 1.0\n",
        "    aji = overall_correct_count / union_pixel_count\n",
        "\n",
        "    return aji.cpu().numpy()"
      ],
      "metadata": {
        "id": "Vb0hrTJ1gIpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision.transforms import transforms\n",
        "import torch\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from skimage import io\n",
        "import sys\n",
        "from scipy.io import loadmat, savemat\n",
        "\n",
        "\n",
        "folder1 = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/unlabelled_img_patches/'\n",
        "num_images = 1000\n",
        "idxs = np.random.RandomState(2023).permutation(5328)\n",
        "\n",
        "OUT_FOLDER = './samp_images'\n",
        "os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "\n",
        "for i in range(num_images):\n",
        "    img_path = folder1 + 'image_' + str(idxs[i]) + '.jpg'\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    OUT_IMAGE_PATH = os.path.join(OUT_FOLDER, 'image_' + str(i) + '.jpg')\n",
        "    cv2.imwrite(OUT_IMAGE_PATH, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
      ],
      "metadata": {
        "id": "0PFzI5PIhOcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18, resnet50\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import functional as F\n",
        "from scipy.io import loadmat\n",
        "import cv2\n",
        "import shutil\n",
        "from matplotlib import pyplot as plt\n",
        "# from simple_colors import *\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/test/metrics.ipynb'\n",
        "%run '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/downstream_train/model.ipynb'\n",
        "\n",
        "from tqdm import tqdm\n",
        "#from metrics import Aggregated_jaccard_index, Hausdorff_distance, Jaccard_score, precision, sensitivity, accuracy, F1_score, conf_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import sys\n",
        "\n",
        "sys.path.append('../downstream_train')\n",
        "#from GenSelfDiff.downstream_train.model import SegNet\n",
        "\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Hyper Parameters\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "IMG_CHANNELS = 3\n",
        "BATCH_SIZE = 1\n",
        "NUM_CLASSES = 4\n",
        "TEST_IMAGE_DIR = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/images'\n",
        "TEST_LABEL_PATH = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/test/label_patches.mat'\n",
        "\n",
        "start = \"\\033[1m\"\n",
        "end = \"\\033[0;0m\"\n",
        "\n",
        "# transformations to be performed on the data points\n",
        "transformations = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def class_weights(target_labl):\n",
        "    _, target_label1 = torch.max(target_labl, dim=0)\n",
        "    weights = np.ones(NUM_CLASSES)\n",
        "    target_label = target_label1.reshape(-1)\n",
        "    all_labels = target_label.cpu().numpy()\n",
        "    labels, label_counts = np.unique(np.array(all_labels), return_counts=True)\n",
        "    w = 1 - np.round(label_counts / np.sum(label_counts), 4)\n",
        "    if len(labels) == 1:\n",
        "        w = 1.0\n",
        "    weights[labels] = w\n",
        "\n",
        "    print(labels)\n",
        "    print(label_counts)\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def get_images_list(path1, k=None):\n",
        "    total_list1 = os.listdir(path1)\n",
        "    total_list1 = sorted(total_list1, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "    if k is None:\n",
        "        return np.array(total_list1)\n",
        "    else:\n",
        "        return np.array(total_list1[:k])\n",
        "\n",
        "\n",
        "class Histo_Dataset(Dataset):\n",
        "    def __init__(self, image1_dir, image1_list, label_list, transform=None):\n",
        "        self.image1_dir = image1_dir\n",
        "        self.image1_list = image1_list\n",
        "        self.label_list = label_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image1_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img1_path = os.path.join(self.image1_dir, self.image1_list[index])\n",
        "\n",
        "        image1 = io.imread(img1_path)\n",
        "        mask = self.label_list[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image1 = self.transform(image1)\n",
        "\n",
        "        return image1, mask\n",
        "\n",
        "\n",
        "def eval_epoch(test_loader, model, num_classes, device):\n",
        "    model_type = 'diffusion'\n",
        "    loss_type = 'SSFL_A'\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        aji_scores = 0.0\n",
        "        jaccard_scores = np.zeros(num_classes-1)\n",
        "        fscores = np.zeros(num_classes-1)\n",
        "        hds = np.zeros(num_classes-1)\n",
        "        accuracies = np.zeros(num_classes-1)\n",
        "        senstivities = np.zeros(num_classes-1)\n",
        "        precisions = np.zeros(num_classes-1)\n",
        "\n",
        "        # jaccard_scores = np.zeros(num_classes)\n",
        "        # fscores = np.zeros(num_classes)\n",
        "        # hds = np.zeros(num_classes)\n",
        "        # accuracies = np.zeros(num_classes)\n",
        "        # senstivities = np.zeros(num_classes)\n",
        "        # precisions = np.zeros(num_classes)\n",
        "\n",
        "        cm = np.zeros((num_classes, num_classes))\n",
        "        total_items = 0\n",
        "        p_bar = tqdm(test_loader)\n",
        "\n",
        "        save_path1 = './true_labels'\n",
        "        save_path2 = './predicted_labels'\n",
        "        save_path3 = './images'\n",
        "\n",
        "        if os.path.exists(save_path1):\n",
        "            shutil.rmtree(save_path1)\n",
        "        os.makedirs(save_path1, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(save_path2):\n",
        "            shutil.rmtree(save_path2)\n",
        "        os.makedirs(save_path2, exist_ok=True)\n",
        "\n",
        "        if os.path.exists(save_path3):\n",
        "            shutil.rmtree(save_path3)\n",
        "        os.makedirs(save_path3, exist_ok=True)\n",
        "\n",
        "        indexing = 0\n",
        "        for img, target_label in p_bar:\n",
        "            img = img.to(device)\n",
        "            target_label = target_label.squeeze(1).to(device)\n",
        "            target_label1 = target_label.long()\n",
        "            target_label = F.one_hot(target_label1, num_classes)\n",
        "            target_label = torch.permute(target_label, (0, 3, 1, 2))\n",
        "\n",
        "            t = torch.full((BATCH_SIZE,), 0, dtype=torch.long)\n",
        "            t = t.to(device)\n",
        "\n",
        "            predicted_label = model(img, t)\n",
        "\n",
        "            aji_score = Aggregated_jaccard_index(target_label, predicted_label, device)\n",
        "            jaccard_score = Jaccard_score(target_label, predicted_label, num_classes)\n",
        "            fscore = F1_score(target_label, predicted_label, num_classes)\n",
        "            hd = Hausdorff_distance(target_label, predicted_label, num_classes)\n",
        "            acc = accuracy(target_label, predicted_label, num_classes)\n",
        "            senstvty = sensitivity(target_label, predicted_label, num_classes)\n",
        "            prec = precision(target_label, predicted_label, num_classes)\n",
        "\n",
        "\n",
        "            print('===============================')\n",
        "            print('           IMAGE_' + str(total_items))\n",
        "            print('===============================')\n",
        "            print('')\n",
        "            print('AJI: ', np.round(aji_score, 4))\n",
        "            print('Jaccard Score: ', np.round(jaccard_score, 4), ' Mean: ',\n",
        "                  np.round(np.mean(jaccard_score), 4))\n",
        "            print('F1 Score: ', np.round(fscore, 4), ' Mean: ', np.round(np.mean(fscore), 4))\n",
        "            print('Hausdorff Distance: ', np.round(hd, 4), ' Mean: ', np.round(np.mean(hd), 4))\n",
        "            print('Accuracy: ', np.round(acc, 4), ' Mean: ', np.round(np.mean(acc), 4))\n",
        "            print('Sensitivity: ', np.round(senstvty, 4), ' Mean: ', np.round(np.mean(senstvty), 4))\n",
        "            print('Precision: ', np.round(prec, 4), ' Mean: ', np.round(np.mean(prec), 4))\n",
        "            print('')\n",
        "\n",
        "            aji_scores += aji_score\n",
        "            jaccard_scores += jaccard_score\n",
        "            fscores += fscore\n",
        "            hds += hd\n",
        "            accuracies += acc\n",
        "            senstivities += senstvty\n",
        "            precisions += prec\n",
        "\n",
        "            cm += conf_matrix(target_label, predicted_label, num_classes)\n",
        "\n",
        "            batch = 1  # predicted_label.shape[0]\n",
        "            total_items += 1  # batch\n",
        "\n",
        "            # True label mappings\n",
        "            labels_t = np.zeros((target_label1.shape[0], target_label1.shape[1], target_label1.shape[2], 3),\n",
        "                                dtype=np.uint8)\n",
        "            # labels_t[target_label1.cpu() == 1] = [127, 255, 0]\n",
        "            labels_t[target_label1.cpu() == 1] = [255, 69, 0]\n",
        "            labels_t[target_label1.cpu() == 2] = [127, 255, 0]\n",
        "            labels_t[target_label1.cpu() == 3] = [135, 206, 250]\n",
        "\n",
        "            # Predicted label mappings\n",
        "            _, pred_labels = torch.max(predicted_label, 1)\n",
        "            labels_p = np.zeros((pred_labels.shape[0], pred_labels.shape[1], pred_labels.shape[2], 3),\n",
        "                                dtype=np.uint8)\n",
        "            # labels_p[pred_labels.cpu() == 1] = [127, 255, 0]\n",
        "            labels_p[pred_labels.cpu() == 1] = [255, 69, 0]\n",
        "            labels_p[pred_labels.cpu() == 2] = [127, 255, 0]\n",
        "            labels_p[pred_labels.cpu() == 3] = [135, 206, 250]\n",
        "\n",
        "            for i in range(target_label.shape[0]):\n",
        "                image_label_t = labels_t[i]\n",
        "                image_label_p = labels_p[i]\n",
        "                out_label_path1 = os.path.join(save_path1, 'label_' + str(indexing) + '_' +\n",
        "                                               str(np.round(np.mean(senstvty) / batch, 2)) + '_' +\n",
        "                                               str(np.round(np.mean(prec) / batch, 2)) + '.jpg')\n",
        "                cv2.imwrite(out_label_path1, cv2.cvtColor(image_label_t, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                out_label_path2 = os.path.join(save_path2, 'label_' + str(indexing) + '_' +\n",
        "                                               str(np.round(np.mean(senstvty) / batch, 2)) + '_' +\n",
        "                                               str(np.round(np.mean(prec) / batch, 2)) + '.jpg')\n",
        "                cv2.imwrite(out_label_path2, cv2.cvtColor(image_label_p, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "                image = torch.permute(img[i], (1, 2, 0))\n",
        "                image = image.cpu().numpy()\n",
        "                image = np.uint8(255 * image)\n",
        "                out_image_path = os.path.join(save_path3, 'image_' + str(indexing) + '_' +\n",
        "                                              str(np.round(np.mean(senstvty) / batch, 2)) + '_' +\n",
        "                                              str(np.round(np.mean(prec) / batch, 2)) + '.jpg')\n",
        "                cv2.imwrite(out_image_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "                wt1 = class_weights(target_label[i])\n",
        "                wt2 = class_weights(predicted_label[i])\n",
        "                temp_t = target_label[i] * predicted_label[i]\n",
        "                # temp_t[temp_t == 0] = 0.5\n",
        "                for cls in range(NUM_CLASSES):\n",
        "                    print([torch.min(temp_t[cls]), torch.max(temp_t[cls])])\n",
        "\n",
        "                indexing += 1\n",
        "\n",
        "            p_bar.set_description()\n",
        "            p_bar.set_postfix(f1_score=np.mean(fscore) / batch, prec=np.mean(prec) / batch,\n",
        "                              acc=np.mean(acc) / batch, max_label=torch.max(target_label1))\n",
        "\n",
        "        print('total items: {}'.format(total_items))\n",
        "        aji_scores = aji_scores/total_items\n",
        "        jaccard_scores = jaccard_scores / total_items\n",
        "        fscores = fscores / total_items\n",
        "        hds = hds/total_items\n",
        "        accuracies = accuracies / total_items\n",
        "        senstivities = senstivities / total_items\n",
        "        precisions = precisions / total_items\n",
        "        cm = np.round(cm / total_items).astype(int)\n",
        "        print('Average AJI: {}'.format(aji_scores))\n",
        "        print('class jaccard scores: {} and Average jaccard score: {}'.format(jaccard_scores,\n",
        "                                                                              np.mean(jaccard_scores)))\n",
        "        print('class F1 scores: {} and Average F1 score: {}'.format(fscores, np.mean(fscores)))\n",
        "        print('class Hausdorff Distances: {} and Average Hausdorff Distance: {}'.format(hds, np.mean(hds)))\n",
        "        print('class accuracies: {} and Average accuracy: {}'.format(accuracies, np.mean(accuracies)))\n",
        "        print('class sensitivities: {} and Average sensitivity: {}'.format(senstivities, np.mean(senstivities)))\n",
        "        print('class precisions: {} and Average precision: {}'.format(precisions, np.mean(precisions)))\n",
        "        disp = ConfusionMatrixDisplay(cm)\n",
        "        disp.plot(cmap='Blues', values_format='')\n",
        "        plt.savefig('./cm_' + model_type + '_' + loss_type + '_' + str(BATCH_SIZE) + '.jpg', dpi=300)\n",
        "\n",
        "\n",
        "def main():\n",
        "    backbone = 'Unet'\n",
        "\n",
        "    # Loading the data\n",
        "    img1_list = get_images_list(TEST_IMAGE_DIR)\n",
        "    label_file = loadmat(TEST_LABEL_PATH)\n",
        "    train_labels = label_file['data']\n",
        "\n",
        "    test_dataset = Histo_Dataset(TEST_IMAGE_DIR, img1_list, train_labels,\n",
        "                                 transform=transformations)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, drop_last=False, num_workers=2)\n",
        "\n",
        "    path_train = '/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/downstream_train/checkpoint.pth'\n",
        "    snapshot = torch.load(path_train)\n",
        "    print(DEVICE)\n",
        "    print(path_train)\n",
        "\n",
        "    model = SegNet(dim=64, channels=3, num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    model.load_state_dict(snapshot['model_state_dict'])\n",
        "\n",
        "    eval_epoch(test_loader, model, NUM_CLASSES, DEVICE)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "EYEJthxMiCpr",
        "outputId": "fe99d0a1-33c5-4795-b180-a3e7fba0158d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2ba9a3b364b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-2ba9a3b364b8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mpath_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/converted_notebooks/GenSelfDiff-HIS-main/GenSelfDiff/downstream_train/checkpoint.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0msnapshot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                 return _legacy_load(\n\u001b[0m\u001b[1;32m   1488\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m                     \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         )\n\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, encoding)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ASCII\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;31m# Risky operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}